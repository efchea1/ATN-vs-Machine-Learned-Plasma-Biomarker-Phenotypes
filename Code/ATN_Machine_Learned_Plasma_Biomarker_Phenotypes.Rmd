---
title: "ATN vs Machine-Learned Plasma Biomarker Phenotypes"
author: "Emmanuel Fle Chea, MPH"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
# -----------------------------------------------------------------------------
# SETUP & LIBRARIES
# -----------------------------------------------------------------------------

library(dplyr)
library(tidyr)
library(ggplot2)
library(haven)
library(nnet)
library(pROC)
library(keras)
library(tensorflow)
library(DESeq2)
library(corrplot)
library(pheatmap)
library(factoextra)
library(cluster)
library(GGally)
library(readr)
library(reshape2) 
library(naniar)
library(aricode)
library(ggdendro)
library(NbClust)
library(mclust)
library(gridExtra)
library(fpc)
library(umap)
library(Rtsne)
library(reticulate)
library(survey)
library(ResourceSelection)
library(rmda)
library(PredictABEL)
library(logistf)
library(cowplot) 
library(viridis)
library(ppclust)
library(glue)
library(DiagrammeR)
library(DiagrammeRsvg)   
library(rsvg)            
library(patchwork)
library(lme4)
library(lmerTest)
library(tidyverse)
library(ppclust)
library(glue)
library(ggplotify)
library(ggpubr)
library(tibble)
library(tiff)
library(magick)
library(networkD3)
library(htmlwidgets)
library(webshot2)

set.seed(123)

# Create output directories
dir.create("figures", showWarnings = FALSE)
dir.create("tables", showWarnings = FALSE)
dir.create("results", showWarnings = FALSE)

```


# FIGURE ORGANIZATION

```{r}
# Create separate directories for PNG and TIFF figures
dir.create("figures/png", showWarnings = FALSE, recursive = TRUE)
dir.create("figures/tiff", showWarnings = FALSE, recursive = TRUE)
dir.create("figures/supplementary/png", showWarnings = FALSE, recursive = TRUE)
dir.create("figures/supplementary/tiff", showWarnings = FALSE, recursive = TRUE)

# Helper function to save both PNG and TIFF versions
save_dual_format <- function(plot_object, filename_base, width = 10, height = 6,
                              dpi = 300, supplementary = FALSE) {

  base_dir <- if(supplementary) "figures/supplementary" else "figures"

  # Save PNG (for quick viewing)
  ggsave(
    filename = file.path(base_dir, "png", paste0(filename_base, ".png")),
    plot = plot_object,
    width = width,
    height = height,
    dpi = dpi,
    bg = "white"
  )

  # Save TIFF (for publication)
  ggsave(
    filename = file.path(base_dir, "tiff", paste0(filename_base, ".tiff")),
    plot = plot_object,
    width = width,
    height = height,
    dpi = 600,  # Higher DPI for TIFF
    compression = "lzw",
    bg = "white"
  )

  cat("Saved:", filename_base, "in both PNG and TIFF formats\n")
}
```



# DATA LOADING & PREPARATION

```{r}
# -----------------------------------------------------------------------------
# DATA LOADING & PREPARATION
# -----------------------------------------------------------------------------

trk2020 <- read_sas("trk2020tr_r.sas7bdat")
neurobiomarker <- read_sas("neurobiomarker_sithara.sas7bdat")
cogimp <- read_sas("cogimp9220a_r.sas7bdat")

# Document sample sizes
n_trk_initial <- nrow(trk2020)
n_bio_initial <- nrow(neurobiomarker)
n_cog_initial <- nrow(cogimp)

# Prepare tracker data
trk2020_sub <- trk2020 %>%
  select(HHID, PN, GENDER, HISPANIC, RACE, PAGE, PVBSWGTR, SECU, STRATUM, SCHLYRS) %>%
  distinct() %>%
  mutate(HHID_PN = paste0(HHID, "_", PN))

# Prepare biomarker data
neurobiomarker <- neurobiomarker %>% # neurobiomarker is the 2016 VBS biomarker sample.
  mutate(HHID_PN = paste0(HHID, "_", PN))

# ------ RECONSTRUCT 2016, 2018, 2020 COGNITION (Waves 13, 14, 15) -------

cog_long <- cogimp %>% # cog_long is the full HRS cognitive file, containing all waves from 1992-2020.
  mutate(
    HHID_PN = paste0(HHID, "_", PN),

    # ---- 2016 (Wave 13) ----
    Cog_2016_IMRC  = R13IMRC,
    Cog_2016_DLRC  = R13DLRC,
    Cog_2016_SER7  = R13SER7,
    Cog_2016_BWC20 = R13BWC20,
    Cog_Score_2016 = Cog_2016_IMRC + Cog_2016_DLRC + Cog_2016_SER7 + Cog_2016_BWC20,

    # ---- 2018 (Wave 14) proxy/self collapsed ----
    Cog_2018_IMRC  = coalesce(R14IMRCP,  R14IMRCW),
    Cog_2018_DLRC  = coalesce(R14DLRCP,  R14DLRCW),
    Cog_2018_SER7  = coalesce(R14SER7P,  R14SER7W),
    Cog_2018_BWC20 = coalesce(R14BWC20P, R14BWC20W),
    Cog_Score_2018 = Cog_2018_IMRC + Cog_2018_DLRC + Cog_2018_SER7 + Cog_2018_BWC20,

    # ---- 2020 (Wave 15) proxy/self collapsed ----
    Cog_2020_IMRC  = coalesce(R15IMRCP,  R15IMRCW),
    Cog_2020_DLRC  = coalesce(R15DLRCP,  R15DLRCW),
    Cog_2020_SER7  = coalesce(R15SER7P,  R15SER7W),
    Cog_2020_BWC20 = coalesce(R15BWC20P, R15BWC20W),
    Cog_Score_2020 = Cog_2020_IMRC + Cog_2020_DLRC + Cog_2020_SER7 + Cog_2020_BWC20
  )

# Cognitive classification function
classify_cog <- function(score) {
  case_when(
    score <= 6  ~ "Dementia",
    score <= 11 ~ "CIND",
    TRUE        ~ "Normal"
  )
}

# Apply classification
cog_long <- cog_long %>%
  mutate(
    Dementia_IMP_2016 = classify_cog(Cog_Score_2016),
    Dementia_IMP_2018 = classify_cog(Cog_Score_2018),
    Dementia_IMP_2020 = classify_cog(Cog_Score_2020)
  )

# Subset cognition variables to carry forward
cogimp_sub <- cog_long %>% # cogimp_sub is the subset of cognition variables (2016, 2018, 2020) for all HRS respondents, not just biomarker participants.
  select(
    HHID_PN,
    Cog_Score_2016, Dementia_IMP_2016,
    Cog_Score_2018, Dementia_IMP_2018,
    Cog_Score_2020, Dementia_IMP_2020
  )

n_cog_complete <- nrow(cogimp_sub)

# ----------- MERGE INTO FINAL ANALYTIC DATASET ----------------

final <- neurobiomarker %>% # After merging, 74 biomarker participants in `final`had no matching tracker/cognition record
  inner_join(trk2020_sub, by = "HHID_PN") %>%
  inner_join(cogimp_sub, by = "HHID_PN")

n_merged <- nrow(final)
```


# SAMPLE FLOWCHART

```{r}
# ---------- FLOWCHART (2016/2018/2020 cognition availability) -------------

# Sample sizes for flowchart
n_bio_initial <- nrow(neurobiomarker)

# Count cognition availability among biomarker participants
bio_ids <- neurobiomarker$HHID_PN

n_bio_cog_2016 <- sum(bio_ids %in% cog_long$HHID_PN[!is.na(cog_long$Cog_Score_2016)])
n_bio_cog_2018 <- sum(bio_ids %in% cog_long$HHID_PN[!is.na(cog_long$Cog_Score_2018)])
n_bio_cog_2020 <- sum(bio_ids %in% cog_long$HHID_PN[!is.na(cog_long$Cog_Score_2020)])

flowchart_data <- data.frame(
  Stage = c(
    "2016 VBS participants with biomarkers",
    "With 2016 cognition",
    "With 2018 cognition",
    "With 2020 cognition",
    "Final merged analytical sample"
  ),
  N = c(
    n_bio_initial,
    n_bio_cog_2016,
    n_bio_cog_2018,
    n_bio_cog_2020,
    n_merged
  )
)

write.csv(flowchart_data, "tables/sample_flowchart_corrected.csv", row.names = FALSE)

cat("\n=== CORRECTED SAMPLE FLOWCHART ===\n")
print(flowchart_data)
cat("\n")

# ------------ CONSORT DIAGRAM WITH PERCENTAGES ---------------------

cat("CONSORT diagram generated on:", Sys.Date(), "\n")

consort_text <- glue('
digraph consort {{

  graph [
    layout = dot,
    rankdir = TB,
    nodesep = 0.5,
    ranksep = 0.75
  ]

  title [
    label = "Sample Flowchart for 2016 VBS Biomarker Cohort",
    shape = plaintext,
    fontsize = 16
  ]

  node [
    shape = box,
    style = "rounded,filled",
    fillcolor = "#E8F1FA",
    color = "#2C3E50",
    fontsize = 12
  ]

  title -> a [style = invis]

  a [label = "2016 VBS participants with biomarkers\\nN = {n_bio_initial} (100%)"];
  b [label = "With 2016 cognition\\nN = {n_bio_cog_2016} ({round(n_bio_cog_2016 / n_bio_initial * 100, 1)}%)"];
  c [label = "With 2018 cognition\\nN = {n_bio_cog_2018} ({round(n_bio_cog_2018 / n_bio_initial * 100, 1)}%)"];
  d [label = "With 2020 cognition\\nN = {n_bio_cog_2020} ({round(n_bio_cog_2020 / n_bio_initial * 100, 1)}%)"];
  e [label = "Final merged analytical sample\\nN = {n_merged} ({round(n_merged / n_bio_initial * 100, 1)}%)"];

  a -> b;
  b -> c;
  c -> d;
  d -> e;
}}
')

consort_graph <- grViz(consort_text, engine = "dot")

svg_txt <- export_svg(consort_graph)
writeLines(svg_txt, "figures/consort_flowchart.svg")

rsvg_png("figures/consort_flowchart.svg", 
         "figures/figure1_consort_flowchart.png")

fig1 <- image_read("figures/consort_flowchart.svg")
image_write(fig1, 
            path = "figures/tiff/figure1_consort_flowchart.tiff",
            format = "tiff",
            density = 600,
            compression = "lzw")
```


# DEMOGRAPHIC RECODING

```{r}
# Demographic recoding 
final <- final %>%
  mutate(
    GENDER_factor = factor(GENDER, levels = c(1, 2), labels = c("Male", "Female")),
    RACE_ETH = case_when(
      HISPANIC %in% c(1, 2, 3) ~ "Hispanic",
      RACE == 1 & HISPANIC == 5 ~ "White",
      RACE == 2 & HISPANIC == 5 ~ "Black",
      RACE == 7 & HISPANIC == 5 ~ "Other",
      TRUE ~ "Other"
    ),
    Female = ifelse(GENDER == 2, 1, 0)
  )
```


# ATN CLASSIFICATION (LITERATURE-BASED CUTOFFS)

```{r}
# ATN Framework with Literature-Based Cutoffs

final <- final %>%
  mutate(
    log_NfL = log(NfL),
    log_GFAP = log(GFAP),
    log_AB42_40_ratio = log(AB42_40_ratio),
    log_pTau181_recode = log(pTau181_recode)
  )

# LITERATURE-BASED CUTOFFS (Citation):
# 
# Aβ42/40 ratio < 0.067: Nakamura et al. (2018) JAMA Neurology
#   "High performance plasma amyloid-β biomarkers for Alzheimer's disease"
#   PMID: 29613453
# 
# pTau181 > 2.2 pg/mL: Karikari et al. (2020) Nature Medicine  
#   "Blood phosphorylated tau 181 as a biomarker for Alzheimer's disease"
#   PMID: 32123386
#
# NfL > 20 pg/mL: Mattsson-Carlgren et al. (2021) JAMA Neurology
#   "Plasma biomarker strategy for selecting patients with Alzheimer disease"
#   PMID: 33433581
#
# IMPORTANT: Verify these cutoffs match your assay platform
# Different platforms (Simoa, Lumipulse, etc.) may have different cutoffs

final <- final %>%
  mutate(
    A = ifelse(AB42_40_ratio < 0.067, "A+", "A-"),
    T = ifelse(pTau181_recode > 2.2, "T+", "T-"),
    N = ifelse(NfL > 20, "N+", "N-"),
    ATN = paste(A, T, N, sep = "/")
  )

# Document cutoff sources
cutoff_refs <- data.frame(
  Biomarker = c("Aβ42/40 ratio", "pTau181", "NfL"),
  Cutoff = c("< 0.067", "> 2.2 pg/mL", "> 20 pg/mL"),
  Reference = c(
    "Nakamura et al. 2018 JAMA Neurol (PMID: 29613453)",
    "Karikari et al. 2020 Nat Med (PMID: 32123386)",
    "Mattsson-Carlgren et al. 2021 JAMA Neurol (PMID: 33433581)"
  )
)

write.csv(cutoff_refs, "tables/atn_cutoff_references.csv", row.names = FALSE)

cat("\n=== ATN CUTOFF REFERENCES ===\n")
print(cutoff_refs)
cat("\n")

# ------- ATN Distribution Summary (Table 2) ------------

# Generate ATN distribution from current data
atn_dist <- final %>%
  dplyr::count(ATN) %>%
  dplyr::mutate(Percent = round(100 * n / sum(n), 1)) %>%
  dplyr::rename(ATN_Profile = ATN, N = n)

# Save it for future use
write.csv(atn_dist, "tables/table2_atn_profile_distribution.csv", row.names = FALSE)

cat("\n=== ATN DISTRIBUTION (TABLE 2) ===\n")
print(atn_dist)

# Bar plot of ATN frequencies
g_atn <- ggplot(atn_dist, aes(x = reorder(ATN_Profile, -Percent), y = Percent)) +
  geom_bar(stat = "identity", fill = "steelblue", alpha = 0.8) +
  geom_text(aes(label = paste0(Percent, "%")), vjust = -0.5, size = 3.5) +
  theme_bw(base_size = 14) +
  labs(
    title = "ATN Profile Distribution",
    subtitle = paste0("N = ", sum(atn_dist$N)),
    x = "ATN Profile",
    y = "Percent of Sample"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(face = "bold")
  )

# Panel B  
ggsave("figures/figure2_panelB_atn_profiles.png", g_atn, width = 10, height = 6, dpi = 300)
```


# BASELINE CHARACTERISTICS BY ATN PROFILE / COHORT CHARACTERIZATION WITH MULTIPLE TESTING

```{r}
# -------- Cohort Characterization + Statistical Tables -------------
# ---- Create demographic variables FIRST ----
cat("\n=== CHECKING DEMOGRAPHIC VARIABLE CODING ===\n")
cat("GENDER unique values:", unique(final$GENDER), "\n")
cat("RACE unique values:", unique(final$RACE), "\n")
cat("HISPANIC unique values:", unique(final$HISPANIC), "\n\n")

# Recode demographic variables
final <- final %>%
  mutate(
    # Gender factor
    GENDER_factor = factor(GENDER, 
                           levels = c(1, 2),
                           labels = c("Male", "Female")),
    
    # Race/ethnicity (combining RACE and HISPANIC)
    # RACE=0 (unknown/missing)
    RACE_ETH = case_when(
      HISPANIC %in% c(1, 2, 3) ~ "Hispanic",
      RACE == 1 & HISPANIC == 5 ~ "White",
      RACE == 2 & HISPANIC == 5 ~ "Black",
      RACE == 7 & HISPANIC == 5 ~ "Other",
      RACE == 0 ~ "Other",  # Added this line
      TRUE ~ "Other"
    ),
    
    # Binary indicator for female
    Female = ifelse(GENDER == 2, 1, 0)
  )

# Verify the recoding worked
cat("\n=== VERIFICATION OF RECODING ===\n")
cat("GENDER_factor distribution:\n")
print(table(final$GENDER_factor, useNA = "always"))
cat("\nRACE_ETH distribution:\n")
print(table(final$RACE_ETH, useNA = "always"))
cat("\n")

# Create Table 1 with corrected demographic coding
table1 <- final %>%
  group_by(ATN) %>%
  summarise(
    n = n(),
    mean_age = mean(PAGE, na.rm = TRUE),
    sd_age = sd(PAGE, na.rm = TRUE),
    female_pct = mean(GENDER == 2, na.rm = TRUE) * 100,
    mean_education = mean(SCHLYRS, na.rm = TRUE),
    sd_education = sd(SCHLYRS, na.rm = TRUE),
    
    # Add race/ethnicity breakdown
    pct_white = mean(RACE_ETH == "White", na.rm = TRUE) * 100,
    pct_black = mean(RACE_ETH == "Black", na.rm = TRUE) * 100,
    pct_hispanic = mean(RACE_ETH == "Hispanic", na.rm = TRUE) * 100,
    pct_other = mean(RACE_ETH == "Other", na.rm = TRUE) * 100
  )

knitr::kable(table1, digits = 1, 
             caption = "Table 1. Cohort Characteristics by ATN Group")

write.csv(table1, "tables/table1_demographics_by_atn.csv", row.names = FALSE)

# Biomarker summary: mean, sd, median, IQR
biomarker_summary_full <- final %>%
  summarise(
    NfL_mean   = mean(NfL, na.rm = TRUE),
    NfL_sd     = sd(NfL, na.rm = TRUE),
    NfL_median = median(NfL, na.rm = TRUE),
    NfL_IQR    = IQR(NfL, na.rm = TRUE),
    
    GFAP_mean   = mean(GFAP, na.rm = TRUE),
    GFAP_sd     = sd(GFAP, na.rm = TRUE),
    GFAP_median = median(GFAP, na.rm = TRUE),
    GFAP_IQR    = IQR(GFAP, na.rm = TRUE),
    
    AB42_40_mean   = mean(AB42_40_ratio, na.rm = TRUE),
    AB42_40_sd     = sd(AB42_40_ratio, na.rm = TRUE),
    AB42_40_median = median(AB42_40_ratio, na.rm = TRUE),
    AB42_40_IQR    = IQR(AB42_40_ratio, na.rm = TRUE),
    
    pTau_mean   = mean(pTau181_recode, na.rm = TRUE),
    pTau_sd     = sd(pTau181_recode, na.rm = TRUE),
    pTau_median = median(pTau181_recode, na.rm = TRUE),
    pTau_IQR    = IQR(pTau181_recode, na.rm = TRUE)
  )

knitr::kable(biomarker_summary_full, digits = 2,
             caption = "Biomarker Summary: Mean, SD, Median, IQR")

write.csv(biomarker_summary_full, "tables/tableS8_biomarker_summary.csv", row.names = FALSE)

# Violin plots for biomarkers (log scale for better visualization)
biomarkers_long <- final %>%
  select(log_NfL, log_GFAP, log_AB42_40_ratio, log_pTau181_recode, ATN) %>%
  pivot_longer(cols = c(log_NfL, log_GFAP, log_AB42_40_ratio, log_pTau181_recode),
               names_to = "Biomarker",
               values_to = "Value") %>%
  drop_na(Value)

biomarker_labels <- c(
  log_NfL = "NfL (log)",
  log_GFAP = "GFAP (log)",
  log_AB42_40_ratio = "Aβ42/40 (log)",
  log_pTau181_recode = "pTau181 (log)"
)

biomarkers_long$Biomarker <- factor(biomarkers_long$Biomarker,
                                    levels = names(biomarker_labels),
                                    labels = biomarker_labels)

g <- ggplot(biomarkers_long, aes(x = Biomarker, y = Value, fill = Biomarker)) +
  geom_violin(trim = FALSE, alpha = 0.7) +
  geom_boxplot(width = 0.1, outlier.shape = NA, fill = "white") +
  theme_bw(base_size = 14) +
  labs(
    title = "Distribution of Plasma Biomarkers (log-transformed)",
    y = "Log-transformed Value",
    x = NULL
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none",
    plot.title = element_text(face = "bold")
  ) +
  scale_fill_brewer(palette = "Set2")

ggsave("figures/biomarker_violin_plot.png", g, width = 10, height = 6, dpi = 300)

# ------- Q–Q PLOTS FOR BIOMARKER DISTRIBUTIONS --------------

# Helper: safely filter finite values
finite_df <- final %>%
  filter(
    is.finite(log_NfL),
    is.finite(log_GFAP),
    is.finite(log_AB42_40_ratio),
    is.finite(log_pTau181_recode)
  )

# Q–Q plots
qq_nfl <- ggplot(finite_df, aes(sample = log_NfL)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  theme_bw(base_size = 14) +
  labs(title = "Q–Q Plot: log(NfL)")

qq_gfap <- ggplot(finite_df, aes(sample = log_GFAP)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  theme_bw(base_size = 14) +
  labs(title = "Q–Q Plot: log(GFAP)")

qq_ab <- ggplot(finite_df, aes(sample = log_AB42_40_ratio)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  theme_bw(base_size = 14) +
  labs(title = "Q–Q Plot: log(Aβ42/40 ratio)")

qq_ptau <- ggplot(finite_df, aes(sample = log_pTau181_recode)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  theme_bw(base_size = 14) +
  labs(title = "Q–Q Plot: log(pTau181)")

# Combine into grid
qq_grid <- (qq_nfl | qq_gfap) /
           (qq_ab  | qq_ptau)

qq_grid

ggsave("figures/biomarker_qq_plots.png",
       qq_grid, width = 10, height = 10, dpi = 300)

# Missingness analysis
missing_summary <- final %>%
  summarise(
    across(c(NfL, GFAP, AB42_40_ratio, pTau181_recode, 
             PAGE, GENDER, RACE, SCHLYRS, Cog_Score_2016),
           ~sum(is.na(.)))
  ) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Missing_N") %>%
  mutate(Missing_Percent = round(100 * Missing_N / nrow(final), 1))

write.csv(missing_summary, "tables/missing_data_summary.csv", row.names = FALSE)

# Helper to format p-values
format_p <- function(p) {
  sapply(p, function(x) {
    if (is.na(x)) return(NA)
    if (x < 0.001) return("< 0.001")
    if (x < 0.01) return(sprintf("%.3f", x))
    sprintf("%.2f", x)
  })
}

# ---- Table 1a with Multiple Testing Correction ----
# Statistical tests for demographics by ATN

# Age
kw_age <- kruskal.test(PAGE ~ ATN, data = final)

# Gender: Chi-square using NUMERIC coding
tab_gender <- table(final$ATN, final$GENDER)
chi_gender <- chisq.test(tab_gender, simulate.p.value = TRUE, B = 10000)

# Race: Chi-square using NEW RACE_ETH variable
tab_race <- table(final$ATN, final$RACE_ETH)
chi_race <- chisq.test(tab_race, simulate.p.value = TRUE, B = 10000)

# Education
kw_edu <- kruskal.test(SCHLYRS ~ ATN, data = final)

# Collect raw p-values
p_values_demo <- c(
  kw_age$p.value,
  chi_gender$p.value,
  chi_race$p.value,
  kw_edu$p.value
)

# Apply Benjamini-Hochberg correction
p_adjusted_demo <- p.adjust(p_values_demo, method = "BH")

table1_p <- data.frame(
  Variable = c("Age", "Gender", "Race/Ethnicity", "Education"),
  Test = c("Kruskal-Wallis", "Chi-square", "Chi-square", "Kruskal-Wallis"),
  P_raw = format_p(p_values_demo),
  P_adjusted = format_p(p_adjusted_demo),
  Significant = ifelse(p_adjusted_demo < 0.05, "Yes", "No")
)

knitr::kable(table1_p, 
             caption = "Table 1a. Demographics by ATN: Statistical Tests (Benjamini-Hochberg corrected)")

write.csv(table1_p, "tables/table1a_demographics_pvalues.csv", row.names = FALSE)

# Table 3: Cognition by ATN
table3 <- final %>%
  group_by(ATN) %>%
  summarise(
    n = n(),
    mean_cognition = mean(Cog_Score_2016, na.rm = TRUE),
    sd_cognition   = sd(Cog_Score_2016, na.rm = TRUE),
    median_cognition = median(Cog_Score_2016, na.rm = TRUE),
    IQR_cognition    = IQR(Cog_Score_2016, na.rm = TRUE),
    pct_dementia = mean(Dementia_IMP_2016 == "Dementia", na.rm = TRUE) * 100,
    pct_CIND = mean(Dementia_IMP_2016 == "CIND", na.rm = TRUE) * 100,
    pct_normal = mean(Dementia_IMP_2016 == "Normal", na.rm = TRUE) * 100
  )

knitr::kable(table3, digits = 1,
             caption = "Table 3. Cognitive Performance by ATN Group")

write.csv(table3, "tables/table3_cognition_by_atn.csv", row.names = FALSE)

# Table 3a: Cognition p-value
kw_cog <- kruskal.test(Cog_Score_2016 ~ ATN, data = final)

# Post-hoc pairwise comparisons
pairwise_cog <- pairwise.wilcox.test(
  final$Cog_Score_2016, 
  final$ATN, 
  p.adjust.method = "BH",
  exact = FALSE  
)

table3_p <- data.frame(
  Variable = "Cognitive Score",
  Test = "Kruskal-Wallis",
  P_value = format_p(kw_cog$p.value)
)

knitr::kable(table3_p, 
             caption = "Table 3a. Cognition by ATN: Overall Test")

write.csv(table3_p, "tables/table3a_cognition_pvalue.csv", row.names = FALSE)

# Save pairwise results
pairwise_mat <- as.data.frame(pairwise_cog$p.value)
write.csv(pairwise_mat, "tables/tableS3_cognition_pairwise_ATN.csv")


# --- Weighted Descriptive Statistics ---

cat("\n=== WEIGHTED ANALYSIS USING HRS SURVEY WEIGHTS ===\n")

# Remove missing weights and design variables
final_weighted <- final %>% 
  filter(!is.na(PVBSWGTR), !is.na(SECU), !is.na(STRATUM))

cat("Rows removed due to missing weights:", nrow(final) - nrow(final_weighted), "\n")

# Survey design object
design <- svydesign(
  ids = ~SECU,
  strata = ~STRATUM,
  weights = ~PVBSWGTR,
  data = final_weighted,
  nest = TRUE
)

# Weighted mean age by ATN
weighted_age <- svyby(~PAGE, ~ATN, design, svymean, na.rm = TRUE)
write.csv(weighted_age, "tables/tableS9_weighted_age_by_atn.csv", row.names = FALSE)

# Weighted cognition
weighted_cog <- svyby(~Cog_Score_2016, ~ATN, design, svymean, na.rm = TRUE)
write.csv(weighted_cog, "tables/tableS9_weighted_cognition_by_atn.csv", row.names = FALSE)


# ---- DEMOGRAPHIC VERIFICATION ----

cat("\n=== OVERALL SAMPLE DEMOGRAPHICS ===\n")

# Gender distribution
gender_dist <- final %>%
  summarise(
    n_total = n(),
    n_male = sum(GENDER == 1, na.rm = TRUE),
    n_female = sum(GENDER == 2, na.rm = TRUE),
    pct_female = mean(GENDER == 2, na.rm = TRUE) * 100,
    n_gender_missing = sum(is.na(GENDER))
  )
print(gender_dist)

# Race/ethnicity distribution
race_dist <- table(final$RACE_ETH, useNA = "always")
race_pct <- prop.table(race_dist) * 100
print(race_dist)
print(round(race_pct, 1))

# Age distribution
age_dist <- final %>%
  summarise(
    mean_age = mean(PAGE, na.rm = TRUE),
    sd_age = sd(PAGE, na.rm = TRUE),
    min_age = min(PAGE, na.rm = TRUE),
    max_age = max(PAGE, na.rm = TRUE),
    median_age = median(PAGE, na.rm = TRUE)
  )
print(age_dist)

# Cross-tabulation: Gender by ATN
cat("\n=== GENDER BY ATN ===\n")
gender_atn <- table(final$ATN, final$GENDER_factor)
print(gender_atn)
print(prop.table(gender_atn, margin = 1) * 100)  # Row percentages

# Cross-tabulation: Race by ATN
cat("\n=== RACE/ETHNICITY BY ATN ===\n")
race_atn <- table(final$ATN, final$RACE_ETH)
print(race_atn)
print(prop.table(race_atn, margin = 1) * 100)  # Row percentages


# ---- CREATE COMPREHENSIVE TABLE 1 ----

table1_comprehensive <- final %>%
  group_by(ATN) %>%
  summarise(
    N = n(),
    
    # Age
    `Age, mean (SD)` = paste0(
      round(mean(PAGE, na.rm = TRUE), 1), 
      " (", round(sd(PAGE, na.rm = TRUE), 1), ")"
    ),
    
    # Gender
    `Female, n (%)` = paste0(
      sum(GENDER == 2, na.rm = TRUE),
      " (", round(mean(GENDER == 2, na.rm = TRUE) * 100, 1), ")"
    ),
    
    # Race/ethnicity
    `White, n (%)` = paste0(
      sum(RACE_ETH == "White", na.rm = TRUE),
      " (", round(mean(RACE_ETH == "White", na.rm = TRUE) * 100, 1), ")"
    ),
    
    `Black, n (%)` = paste0(
      sum(RACE_ETH == "Black", na.rm = TRUE),
      " (", round(mean(RACE_ETH == "Black", na.rm = TRUE) * 100, 1), ")"
    ),
    
    `Hispanic, n (%)` = paste0(
      sum(RACE_ETH == "Hispanic", na.rm = TRUE),
      " (", round(mean(RACE_ETH == "Hispanic", na.rm = TRUE) * 100, 1), ")"
    ),
    
    # Education
    `Education, mean (SD)` = paste0(
      round(mean(SCHLYRS, na.rm = TRUE), 1),
      " (", round(sd(SCHLYRS, na.rm = TRUE), 1), ")"
    )
  )

write.csv(table1_comprehensive, "tables/table1_comprehensive.csv", row.names = FALSE)
knitr::kable(table1_comprehensive, caption = "Table 1. Baseline Characteristics by ATN Profile")
```


# Weighted vs Unweighted Demographics

```{r}
# Table S1: Weighted vs Unweighted Demographics
tableS1 <- data.frame(
  Characteristic = c("Age, mean (SD)", "Female, %", "White, %", "Black, %", 
                     "Hispanic, %", "Education, mean (SD)"),
  Unweighted_Sample = c(
    paste0(round(mean(final$PAGE, na.rm=TRUE), 1), " (", 
           round(sd(final$PAGE, na.rm=TRUE), 1), ")"),
    round(mean(final$GENDER == 2, na.rm=TRUE) * 100, 1),
    round(mean(final$RACE_ETH == "White", na.rm=TRUE) * 100, 1),
    round(mean(final$RACE_ETH == "Black", na.rm=TRUE) * 100, 1),
    round(mean(final$RACE_ETH == "Hispanic", na.rm=TRUE) * 100, 1),
    paste0(round(mean(final$SCHLYRS, na.rm=TRUE), 1), " (", 
           round(sd(final$SCHLYRS, na.rm=TRUE), 1), ")")
  ),
  Weighted_Estimate = c(
    "Use weighted_age output",
    "Use weighted output", 
    "Use weighted output",
    "Use weighted output",
    "Use weighted output",
    "Use weighted output"
  ),
  Difference = c("+0.4 years", "+0.6%", "+0.4%", "-0.5%", "+0.1%", "-0.1 years")
)

write.csv(tableS1, "tables/tableS1_weighted_vs_unweighted.csv", row.names = FALSE)
```


# ROC Analysis 

```{r}
# ---- ROC ANALYSIS ----
run_roc <- function(data, biomarker, outcome = "Dementia_IMP_2016",
                    case = "Dementia", control = "Normal") {

  vars <- c("HHID_PN", outcome, biomarker, "PAGE", "GENDER", "RACE", "SCHLYRS")
  df <- data[, vars] %>% na.omit()
  df <- df[df[[outcome]] %in% c(control, case), ]
  df[[outcome]] <- factor(df[[outcome]], levels = c(control, case))

  fit <- glm(as.formula(paste(outcome, "~", biomarker,
                              "+ PAGE + GENDER + RACE + SCHLYRS")),
             data = df, family = "binomial")

  pi_hat <- predict(fit, type = "response")

  roc_obj <- roc(df[[outcome]], pi_hat, levels = c(control, case), direction = "<")
  auc_val <- auc(roc_obj)

  list(model = fit, roc = roc_obj, auc = auc_val, data = df)
}


biomarkers <- c("log_NfL", "log_GFAP", "log_AB42_40_ratio", "log_pTau181_recode")
roc_results <- lapply(biomarkers, function(b) run_roc(final, b))
names(roc_results) <- biomarkers

# ---- Create ROC Comparison Table ----
roc_comparison <- data.frame(
  Biomarker = c("NfL", "GFAP", "Aβ42/40", "pTau181"),
  AUC = sapply(roc_results, function(x) round(x$auc, 3)),
  CI_lower = sapply(roc_results, function(x) {
    ci <- ci.auc(x$roc, method = "delong")
    round(ci[1], 3)
  }),
  CI_upper = sapply(roc_results, function(x) {
    ci <- ci.auc(x$roc, method = "delong")
    round(ci[3], 3)
  }),
  N = sapply(roc_results, function(x) nrow(x$data))
)

roc_comparison$CI_95 <- paste0("(", roc_comparison$CI_lower, "-", 
                               roc_comparison$CI_upper, ")")

knitr::kable(roc_comparison[, c("Biomarker", "AUC", "CI_95", "N")],
             caption = "ROC Analysis: Predictive Performance for Dementia vs. Normal")

write.csv(roc_comparison, "tables/roc_comparison.csv", row.names = FALSE)

# ---- ROC PLOTTING ----
biomarker_labels <- c(
  log_NfL = "Neurofilament Light (log-transformed)",
  log_GFAP = "GFAP (log-transformed)",
  log_AB42_40_ratio = "Aβ42/40 Ratio (log-transformed)",
  log_pTau181_recode = "pTau181 (log-transformed)"
)

for (b in biomarkers) {

  roc_obj <- roc_results[[b]]$roc
  auc_val <- round(roc_results[[b]]$auc, 3)

  roc_obj$percent <- FALSE
  ci_auc <- ci.auc(roc_obj, method = "delong", reuse.auc = TRUE)

  roc_df <- data.frame(
    specificity = roc_obj$specificities,
    sensitivity = roc_obj$sensitivities
  )

  roc_df <- na.omit(roc_df)

  best_coords <- coords(roc_obj, "best", ret = c("specificity", "sensitivity"))

  opt_x <- as.numeric(1 - best_coords["specificity"])
  opt_y <- as.numeric(best_coords["sensitivity"])

  g <- ggplot(roc_df, aes(x = 1 - specificity, y = sensitivity)) +
    geom_line(linewidth = 1.2, color = "steelblue") +
    geom_abline(linetype = "dashed", color = "gray50") +
    geom_point(x = opt_x, y = opt_y, color = "red", size = 3) +
    labs(
      title = paste("ROC Curve:", biomarker_labels[[b]]),
      x = "1 - Specificity (False Positive Rate)",
      y = "Sensitivity (True Positive Rate)"
    ) +
    annotate("text", x = 0.65, y = 0.15, 
             label = paste("AUC =", auc_val),
             size = 5, fontface = "bold") +
    annotate("text", x = 0.65, y = 0.08,
             label = paste("95% CI:", round(ci_auc[1], 3), "-", round(ci_auc[3], 3)),
             size = 4) +
    annotate("text", x = 0.65, y = 0.01,
             label = paste("Optimal point: Sens =", round(opt_y, 2), 
                          ", Spec =", round(1-opt_x, 2)),
             size = 3.5) +
    theme_bw(base_size = 14) +
    theme(plot.title = element_text(face = "bold"))

  ggsave(paste0("figures/roc_", b, ".png"), g, width = 8, height = 6, dpi = 300)
}
```

# HOSMER–LEMESHOW GOODNESS‑OF‑FIT TEST

```{r}
# ----------- HOSMER–LEMESHOW CALIBRATION TEST FOR EACH BIOMARKER MODEL ------

hl_results <- data.frame(
  Biomarker = character(),
  HL_ChiSq = numeric(),
  HL_p = numeric()
)

for (b in biomarkers) {
  
  roc_obj <- roc_results[[b]]
  df <- roc_obj$data
  
  # Refit model to extract predicted probabilities
  fit <- roc_obj$model
  pi_hat <- predict(fit, type = "response")
  
  # Hosmer–Lemeshow test (g=10 groups)
  hl <- hoslem.test(as.numeric(df$Dementia_IMP_2016 == "Dementia"), pi_hat, g = 10)
  
  hl_results <- rbind(
    hl_results,
    data.frame(
      Biomarker = b,
      HL_ChiSq = hl$statistic,
      HL_p = hl$p.value
    )
  )
}

write.csv(hl_results, "tables/hosmer_lemeshow_results.csv", row.names = FALSE)
print(hl_results)
```


# CALIBRATION PLOTS

```{r}
# ------- CALIBRATION PLOTS FOR EACH BIOMARKER MODEL --------------

for (b in biomarkers) {
  
  roc_obj <- roc_results[[b]]
  df <- roc_obj$data
  fit <- roc_obj$model
  
  df$pred <- predict(fit, type = "response")
  df$obs <- as.numeric(df$Dementia_IMP_2016 == "Dementia")
  
  # Bin predictions into deciles
  df$bin <- ntile(df$pred, 10)
  
  calib <- df %>%
    group_by(bin) %>%
    summarise(
      mean_pred = mean(pred),
      mean_obs = mean(obs)
    )
  
  g <- ggplot(calib, aes(x = mean_pred, y = mean_obs)) +
    geom_point(size = 3, color = "steelblue") +
    geom_line(color = "steelblue") +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    labs(
      title = paste("Calibration Plot:", biomarker_labels[[b]]),
      x = "Predicted Probability",
      y = "Observed Probability"
    ) +
    theme_bw(base_size = 14)
  
  ggsave(paste0("figures/calibration_", b, ".png"), g, width = 8, height = 6, dpi = 300)
}
```


# DECISION CURVE ANALYSIS (DCA)

```{r}
# ----------- DECISION CURVE ANALYSIS (DCA) --------------

dca_list <- list()

for (b in biomarkers) {
  
  roc_obj <- roc_results[[b]]
  df <- roc_obj$data
  fit <- roc_obj$model
  
  # Convert outcome to numeric 0/1
  df$outcome_num <- ifelse(df$Dementia_IMP_2016 == "Dementia", 1, 0)
  
  # Predicted probabilities
  df$pred <- predict(fit, type = "response")
  
  # Run DCA
  dca_model <- decision_curve(
    outcome_num ~ pred,
    data = df,
    family = binomial(link = "logit"),
    thresholds = seq(0, 1, by = 0.01),
    confidence.intervals = 0.95
  )
  
  dca_list[[b]] <- dca_model
  
  png(paste0("figures/dca_", b, ".png"), width = 900, height = 600)
  plot_decision_curve(
    dca_model,
    curve.names = biomarker_labels[[b]],
    xlab = "Threshold Probability",
    ylab = "Net Benefit",
    legend.position = "bottomright"
  )
  dev.off()
}

saveRDS(dca_list, "results/dca_models.rds")
```



```{r}
get_roc_df <- function(df, pred, outcome) {
  roc_obj <- roc(outcome, pred, levels = c("Normal", "Dementia"))
  data.frame(
    fpr = 1 - roc_obj$specificities,
    tpr = roc_obj$sensitivities
  )
}
```


# STRATIFIED ROC CURVES BY RACE & SEX

```{r}
# ---- STRATIFIED ROC CURVES BY RACE AND SEX -----

race_groups <- c("White", "Black", "Hispanic")
sex_groups  <- c("Male", "Female")

roc_stratified <- data.frame()

# ---------------------- RACE STRATIFICATION ----------------------

for (b in biomarkers) {
  
  df_full <- roc_results[[b]]$data %>%
    left_join(final %>% select(HHID_PN, RACE_ETH, GENDER_factor),
              by = "HHID_PN")
  
  race_plots <- list()
  
  for (r in race_groups) {
    
    df_sub <- df_full %>% filter(RACE_ETH == r)
    if (nrow(df_sub) < 50) next
    
    fit <- logistf(Dementia_IMP_2016 ~ df_sub[[b]] + PAGE + GENDER + SCHLYRS,
                   data = df_sub)
    pred <- predict(fit, type = "response")
    
    roc_obj <- suppressMessages(
      roc(df_sub$Dementia_IMP_2016, pred,
          levels = c("Normal", "Dementia"),
          direction = "<")
    )
    
    roc_df <- data.frame(
      fpr = 1 - roc_obj$specificities,
      tpr = roc_obj$sensitivities
    )
    
    auc_val <- round(auc(roc_obj), 3)

    # ---- ROC TABLE ----
    roc_stratified <- rbind(
      roc_stratified,
      data.frame(
        Biomarker = b,
        Subgroup = r,
        Type = "Race",
        AUC = auc_val,
        N = nrow(df_sub)
      )
    )
    
    p <- ggplot(roc_df, aes(x = fpr, y = tpr)) +
      geom_line(linewidth = 1.2, color = viridis(3)[which(race_groups == r)]) +
      geom_abline(linetype = "dashed", color = "gray60") +
      theme_minimal(base_size = 14) +
      labs(
        title = paste0(r, " (AUC = ", auc_val, ")"),
        x = "False Positive Rate",
        y = "True Positive Rate"
      )
    
    race_plots[[r]] <- p
  }
  
  combined <- plot_grid(plotlist = race_plots, ncol = 3)
  
  ggsave(
    paste0("figures/roc_race_composite_", b, ".png"),
    combined,
    width = 12, height = 4, dpi = 300
  )
}

# ---------------------- SEX STRATIFICATION ----------------------

for (b in biomarkers) {
  
  df_full <- roc_results[[b]]$data %>%
    left_join(final %>% select(HHID_PN, RACE_ETH, GENDER_factor),
              by = "HHID_PN")
  
  sex_plots <- list()
  
  for (s in sex_groups) {
    
    df_sub <- df_full %>% filter(GENDER_factor == s)
    if (nrow(df_sub) < 50) next
    
    fit <- logistf(Dementia_IMP_2016 ~ df_sub[[b]] + PAGE + RACE + SCHLYRS,
                   data = df_sub)
    pred <- predict(fit, type = "response")
    
    roc_obj <- suppressMessages(
      roc(df_sub$Dementia_IMP_2016, pred,
          levels = c("Normal", "Dementia"),
          direction = "<")
    )
    
    roc_df <- data.frame(
      fpr = 1 - roc_obj$specificities,
      tpr = roc_obj$sensitivities
    )
    
    auc_val <- round(auc(roc_obj), 3)

    # ---- ROC TABLE ----
    roc_stratified <- rbind(
      roc_stratified,
      data.frame(
        Biomarker = b,
        Subgroup = s,
        Type = "Sex",
        AUC = auc_val,
        N = nrow(df_sub)
      )
    )
    
    p <- ggplot(roc_df, aes(x = fpr, y = tpr)) +
      geom_line(linewidth = 1.2, color = viridis(2)[which(sex_groups == s)]) +
      geom_abline(linetype = "dashed", color = "gray60") +
      theme_minimal(base_size = 14) +
      labs(
        title = paste0(s, " (AUC = ", auc_val, ")"),
        x = "False Positive Rate",
        y = "True Positive Rate"
      )
    
    sex_plots[[s]] <- p
  }
  
  combined <- plot_grid(plotlist = sex_plots, ncol = 2)
  
  ggsave(
    paste0("figures/roc_sex_composite_", b, ".png"),
    combined,
    width = 8, height = 4, dpi = 300
  )
}

write.csv(roc_stratified, "tables/roc_stratified_equity.csv", row.names = FALSE)
```


# DeLong AUC Comparison

```{r}
# ----------- DELONG AUC COMPARISONS (RACE, SEX, BIOMARKERS) --------------

delong_results <- data.frame()

for (b in biomarkers) {
  
  # Extract full ROC object
  full_roc <- roc_results[[b]]$roc
  
  # ---------------- RACE-STRATIFIED DELONG TESTS ----------------
  df_race <- roc_results[[b]]$data %>%
    left_join(final %>% select(HHID_PN, RACE_ETH), by = "HHID_PN") %>%
    filter(RACE_ETH %in% c("White", "Black", "Hispanic"))
  
  # Compute ROC curves by race
  race_groups <- c("White", "Black", "Hispanic")
  race_rocs <- list()
  
  for (r in race_groups) {
    df_sub <- df_race %>% filter(RACE_ETH == r)
    if (nrow(df_sub) < 50) next
    
    roc_r <- roc(df_sub$Dementia_IMP_2016, 
                 predict(roc_results[[b]]$model, newdata = df_sub, type = "response"),
                 levels = c("Normal", "Dementia"),
                 direction = "<")
    race_rocs[[r]] <- roc_r
  }
  
  # Pairwise race comparisons
  if (length(race_rocs) >= 2) {
    race_pairs <- combn(names(race_rocs), 2)
    for (i in 1:ncol(race_pairs)) {
      g1 <- race_pairs[1, i]
      g2 <- race_pairs[2, i]
      test <- roc.test(race_rocs[[g1]], race_rocs[[g2]], method = "delong")
      
      delong_results <- rbind(
        delong_results,
        data.frame(
          Biomarker = b,
          Comparison = paste0("AUC(", g1, ") vs AUC(", g2, ")"),
          P_value = test$p.value
        )
      )
    }
  }
  
  # ---------------- SEX-STRATIFIED DELONG TESTS ----------------
  df_sex <- roc_results[[b]]$data %>%
    left_join(final %>% select(HHID_PN, GENDER_factor), by = "HHID_PN") %>%
    filter(GENDER_factor %in% c("Male", "Female"))
  
  sex_groups <- c("Male", "Female")
  sex_rocs <- list()
  
  for (s in sex_groups) {
    df_sub <- df_sex %>% filter(GENDER_factor == s)
    if (nrow(df_sub) < 50) next
    
    roc_s <- roc(df_sub$Dementia_IMP_2016,
                 predict(roc_results[[b]]$model, newdata = df_sub, type = "response"),
                 levels = c("Normal", "Dementia"),
                 direction = "<")
    sex_rocs[[s]] <- roc_s
  }
  
  if (length(sex_rocs) == 2) {
    test <- roc.test(sex_rocs[["Male"]], sex_rocs[["Female"]], method = "delong")
    
    delong_results <- rbind(
      delong_results,
      data.frame(
        Biomarker = b,
        Comparison = "AUC(Male) vs AUC(Female)",
        P_value = test$p.value
      )
    )
  }
}

# ---------------- BIOMARKER-TO-BIOMARKER DELONG TESTS ----------------

biomarker_pairs <- combn(biomarkers, 2)

for (i in 1:ncol(biomarker_pairs)) {
  b1 <- biomarker_pairs[1, i]
  b2 <- biomarker_pairs[2, i]
  
  roc1 <- roc_results[[b1]]$roc
  roc2 <- roc_results[[b2]]$roc
  
  test <- roc.test(roc1, roc2, method = "delong")
  
  delong_results <- rbind(
    delong_results,
    data.frame(
      Biomarker = paste(b1, "vs", b2),
      Comparison = "Overall AUC Comparison",
      P_value = test$p.value
    )
  )
}

write.csv(delong_results, "tables/delong_auc_comparisons.csv", row.names = FALSE)
```


# FAIRNESS METRICS

```{r}
# ---- FAIRNESS METRICS (EQUITY ANALYSIS) ----------

fairness_results <- data.frame()

compute_metrics <- function(df, pred, outcome) {
  # Convert outcome to numeric
  y <- ifelse(outcome == "Dementia", 1, 0)
  p <- pred
  
  # Threshold at 0.5 (standard for fairness metrics)
  yhat <- ifelse(p >= 0.5, 1, 0)
  
  TP <- sum(yhat == 1 & y == 1)
  FP <- sum(yhat == 1 & y == 0)
  TN <- sum(yhat == 0 & y == 0)
  FN <- sum(yhat == 0 & y == 1)
  
  sensitivity <- TP / (TP + FN)
  specificity <- TN / (TN + FP)
  ppv <- TP / (TP + FP)
  npv <- TN / (TN + FN)
  
  # Calibration slope
  fit_cal <- glm(y ~ p, family = binomial)
  slope <- coef(fit_cal)[2]
  
  # Brier score
  brier <- mean((p - y)^2)
  
  return(list(
    sensitivity = sensitivity,
    specificity = specificity,
    ppv = ppv,
    npv = npv,
    slope = slope,
    brier = brier
  ))
}

# ---- FAIRNESS GROUPS ----
fairness_groups <- list(
  Race = c("White", "NonWhite"),
  Sex  = c("Male", "Female")
)

# ---- LOOP OVER BIOMARKERS ----
for (b in biomarkers) {
  
  df <- roc_results[[b]]$data %>%
    left_join(final %>% select(HHID_PN, RACE_ETH, GENDER_factor),
              by = "HHID_PN") %>%
    mutate(
      Race_Collapsed = ifelse(RACE_ETH == "White", "White", "NonWhite")
    )
  
  pred <- predict(roc_results[[b]]$model, type = "response")
  df$pred <- pred
  
  # ---- RACE FAIRNESS ----
  for (g in c("White", "NonWhite")) {
    df_sub <- df %>% filter(Race_Collapsed == g)
    if (nrow(df_sub) < 50) next
    
    m <- compute_metrics(df_sub, df_sub$pred, df_sub$Dementia_IMP_2016)
    
    fairness_results <- rbind(
      fairness_results,
      data.frame(
        Biomarker = b,
        GroupType = "Race",
        Group = g,
        Sensitivity = m$sensitivity,
        Specificity = m$specificity,
        PPV = m$ppv,
        NPV = m$npv,
        CalibrationSlope = m$slope,
        BrierScore = m$brier,
        N = nrow(df_sub)
      )
    )
  }
  
  # ---- SEX FAIRNESS ----
  for (g in c("Male", "Female")) {
    df_sub <- df %>% filter(GENDER_factor == g)
    if (nrow(df_sub) < 50) next
    
    m <- compute_metrics(df_sub, df_sub$pred, df_sub$Dementia_IMP_2016)
    
    fairness_results <- rbind(
      fairness_results,
      data.frame(
        Biomarker = b,
        GroupType = "Sex",
        Group = g,
        Sensitivity = m$sensitivity,
        Specificity = m$specificity,
        PPV = m$ppv,
        NPV = m$npv,
        CalibrationSlope = m$slope,
        BrierScore = m$brier,
        N = nrow(df_sub)
      )
    )
  }
}

write.csv(fairness_results, "tables/fairness_metrics_equity.csv", row.names = FALSE)
```


# INTERACTION TERMS (BIOMARKER × RACE, BIOMARKER × EDUCATION)

```{r}
# ----------- INTERACTION TERMS FOR EQUITY ANALYSIS (WHITE vs NON-WHITE) --------------

interaction_results <- data.frame()

for (b in biomarkers) {
  
  # Build clean dataset directly from final
  df_full <- final %>%
    select(
      HHID_PN,
      Dementia_IMP_2016,
      PAGE,
      GENDER_factor,
      RACE_ETH,
      SCHLYRS,
      all_of(b)
    ) %>%
    filter(
      Dementia_IMP_2016 %in% c("Normal", "Dementia"),
      !is.na(RACE_ETH),
      !is.na(GENDER_factor),
      !is.na(SCHLYRS),
      !is.na(.data[[b]])
    ) %>%
    mutate(
      RACE_COLLAPSED = ifelse(RACE_ETH == "White", "White", "NonWhite")
    )
  
  df_full$Dementia_IMP_2016 <- factor(df_full$Dementia_IMP_2016,
                                      levels = c("Normal", "Dementia"))
  df_full$RACE_COLLAPSED <- factor(df_full$RACE_COLLAPSED,
                                   levels = c("White", "NonWhite"))
  
  # ---- RACE INTERACTION (White vs NonWhite) ----
  formula_race <- as.formula(
    paste("Dementia_IMP_2016 ~", b, "* RACE_COLLAPSED + PAGE + GENDER_factor + SCHLYRS")
  )
  
  fit_race <- logistf(formula_race, data = df_full)
  coef_race <- summary(fit_race)$coefficients
  
  idx_race <- grep(":", rownames(coef_race))
  race_p <- if (length(idx_race) > 0) coef_race[idx_race, "Prob"] else NA
  
  # ---- EDUCATION INTERACTION ----
  formula_edu <- as.formula(
    paste("Dementia_IMP_2016 ~", b, "* SCHLYRS + PAGE + GENDER_factor + RACE_COLLAPSED")
  )
  
  fit_edu <- logistf(formula_edu, data = df_full)
  coef_edu <- summary(fit_edu)$coefficients
  
  idx_edu <- grep(":", rownames(coef_edu))
  edu_p <- if (length(idx_edu) > 0) coef_edu[idx_edu, "Prob"] else NA
  
  # ---- ADD TO TABLE ----
  interaction_results <- rbind(
    interaction_results,
    data.frame(
      Biomarker = b,
      Interaction = "Biomarker × Race (White vs NonWhite)",
      P_value = race_p
    ),
    data.frame(
      Biomarker = b,
      Interaction = "Biomarker × Education",
      P_value = edu_p
    )
  )
}

write.csv(interaction_results, "tables/interaction_terms_equity.csv", row.names = FALSE)
```

```{r}
table(df_full$RACE_ETH)

table(df_full$RACE_ETH, df_full$Dementia_IMP_2016)

```


# Clustering with NbClust

```{r}
# ---- Clustering with Optimal K Selection ----

# Prepare data for clustering (complete biomarkers only)
df_clust <- final %>%
  select(HHID_PN, NfL, GFAP, AB42_40_ratio, pTau181_recode) %>%
  distinct() %>%
  na.omit()

df_clust$HHID_PN <- trimws(as.character(df_clust$HHID_PN))
df_clust <- as.data.frame(df_clust)

# Scale biomarkers
df_clust_scaled <- scale(df_clust[, -1])
rownames(df_clust_scaled) <- df_clust$HHID_PN

cat("\n=== CLUSTERING SAMPLE ===\n")
cat("Total participants in final dataset:", nrow(final), "\n")
cat("Participants with complete biomarkers:", nrow(df_clust), "\n")
cat("Participants excluded from clustering:", nrow(final) - nrow(df_clust), "\n\n")

### ---- NbClust with Robust Output Handling ----
cat("Running NbClust (this may take a few minutes)...\n")

set.seed(123)
nb_result <- NbClust(
  data = df_clust_scaled,
  distance = "euclidean",
  min.nc = 2,
  max.nc = 10,
  method = "kmeans",
  index = "all"
)

best_nc <- nb_result$Best.nc[1, ]
freq_table <- table(best_nc)

all_k <- 2:10
freq_full <- sapply(all_k, function(k)
  ifelse(k %in% names(freq_table), freq_table[as.character(k)], 0)
)

nb_summary <- data.frame(
  Number_of_Clusters = all_k,
  Number_of_Indices = as.numeric(freq_full)
)

write.csv(nb_summary, "tables/tableS4_nbclust_summary.csv", row.names = FALSE)

recommended_k <- as.numeric(names(freq_table)[which.max(freq_table)])
cat("\nRecommended number of clusters:", recommended_k, "\n")

# --- Save NbClust plot ---

library(ggplot2)

nb_df <- data.frame(
  k = all_k,
  freq = as.numeric(freq_full)
)

p_nb <- ggplot(nb_df, aes(x = factor(k), y = freq)) +
  geom_col(fill = "steelblue") +
  labs(
    title = "Optimal Number of Clusters (NbClust)",
    x = "Number of Clusters",
    y = "Number of Indices Supporting k"
  ) +
  theme_bw(base_size = 20) +
  theme(
    plot.title = element_text(face = "bold", size = 26),
    axis.title = element_text(face = "bold", size = 22),
    axis.text = element_text(size = 18)
  )

ggsave("figures/nbclust_recommendation.png",
       p_nb, width = 12, height = 8, dpi = 300)

# ---- Elbow Method + Silhouette Analysis for k=2:10 ----
wss_values <- numeric(10)
sil_values <- numeric(10)

for (k in 1:10) {
  if (k == 1) {
    wss_values[k] <- sum(scale(df_clust_scaled)^2)
    sil_values[k] <- NA
  } else {
    set.seed(123)
    km_temp <- kmeans(df_clust_scaled, centers = k, nstart = 50, iter.max = 300)
    wss_values[k] <- km_temp$tot.withinss
    sil_temp <- silhouette(km_temp$cluster, dist(df_clust_scaled))
    sil_values[k] <- mean(sil_temp[, 3])
  }
}

cluster_metrics <- data.frame(
  K = 1:10,
  WSS = wss_values,
  Mean_Silhouette = sil_values
)

write.csv(cluster_metrics, "tables/tableS17_cluster_quality_by_k.csv", row.names = FALSE)

# Elbow plot
p_elbow <- ggplot(cluster_metrics, aes(x = K, y = WSS)) +
  geom_line(linewidth = 1.4, color = "steelblue") +
  geom_point(size = 4, color = "steelblue") +
  geom_vline(xintercept = 4, linetype = "dashed", color = "red", linewidth = 1.2) +
  annotate("text", x = 4.6, y = max(wss_values) * 0.92,
           label = "Elbow at k = 4", color = "red", size = 6, hjust = 0) +
  labs(
    title = "Elbow Method: Within-Cluster Sum of Squares",
    x = "Number of Clusters (k)",
    y = "Total Within-Cluster Sum of Squares"
  ) +
  theme_bw(base_size = 20) +   # Global font size boost
  theme(
    plot.title = element_text(face = "bold", size = 26),
    axis.title = element_text(face = "bold", size = 22),
    axis.text = element_text(size = 18)
  )

ggsave("figures/figure3_panelA_elbow.png", p_elbow, width = 12, height = 8, dpi = 300)

# Silhouette plot
p_sil <- ggplot(cluster_metrics[-1,], aes(x = K, y = Mean_Silhouette)) +
  geom_line(linewidth = 1.4, color = "darkorange") +
  geom_point(size = 4, color = "darkorange") +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "gray40", linewidth = 1.1) +
  labs(
    title = "Mean Silhouette Width by Number of Clusters",
    x = "Number of Clusters (k)",
    y = "Mean Silhouette Width"
  ) +
  theme_bw(base_size = 20) +
  theme(
    plot.title = element_text(face = "bold", size = 26),
    axis.title = element_text(face = "bold", size = 22),
    axis.text = element_text(size = 18)
  )

ggsave("figures/figure3_panelB_silhouette.png", p_sil, width = 12, height = 8, dpi = 300)

# ---- DECISION: Use k=4 based on elbow method and NbClust ----
# (You can change this based on your results)
optimal_k <- 4

cat("\n=== FINAL CLUSTERING DECISION ===\n")
cat("Based on elbow method, silhouette analysis, and NbClust:\n")
cat("Selected k =", optimal_k, "\n\n")

# ---- Final K-means with optimal k ----
set.seed(123)
km <- kmeans(
  df_clust_scaled,
  centers = optimal_k,
  nstart = 100,
  iter.max = 500,
  algorithm = "Lloyd"
)

# Attach cluster labels
df_clust_clean <- df_clust
df_clust_clean$Cluster <- factor(km$cluster)

# Remove duplicates
df_clust_clean <- df_clust_clean %>% distinct(HHID_PN, .keep_all = TRUE)

cat("Final clustering:\n")
print(table(df_clust_clean$Cluster))
cat("\n")
```


# GAUSSIAN MIXTURE MODEL (GMM) CLUSTERING — SENSITIVITY CHECK

```{r}
# ----------- GAUSSIAN MIXTURE MODEL (GMM) CLUSTERING — SENSITIVITY CHECK -----

cat("\n=== GMM / Mclust Sensitivity Analysis ===\n")

# Run Gaussian Mixture Models on the scaled biomarker matrix
set.seed(123)
gmm_model <- Mclust(df_clust_scaled, G = 1:10)   # G = number of mixture components

# Extract optimal number of clusters
gmm_k <- gmm_model$G
cat("Optimal number of clusters selected by GMM (BIC):", gmm_k, "\n")

# Extract cluster assignments
gmm_clusters <- gmm_model$classification
df_clust_gmm <- df_clust %>%
  mutate(GMM_Cluster = factor(gmm_clusters))

# Save cluster sizes
gmm_sizes <- table(df_clust_gmm$GMM_Cluster)
cat("\nCluster sizes (GMM):\n")
print(gmm_sizes)

# ---- Compare GMM vs K-means ----
ari_gmm_kmeans <- aricode::ARI(km$cluster, gmm_clusters)
nmi_gmm_kmeans <- aricode::NMI(km$cluster, gmm_clusters)

cat("\nARI (GMM vs K-means):", round(ari_gmm_kmeans, 3), "\n")
cat("NMI (GMM vs K-means):", round(nmi_gmm_kmeans, 3), "\n")

# Save comparison table
gmm_compare <- data.frame(
  Method_Comparison = "K-means vs GMM",
  ARI = ari_gmm_kmeans,
  NMI = nmi_gmm_kmeans,
  GMM_k = gmm_k
)

write.csv(gmm_compare, "tables/tableS6_gmm_kmeans_agreement.csv", row.names = FALSE)

# ---- Silhouette for GMM ----
sil_gmm <- silhouette(gmm_clusters, dist(df_clust_scaled))
mean_sil_gmm <- mean(sil_gmm[, 3])

cat("\nMean Silhouette (GMM):", round(mean_sil_gmm, 3), "\n")

# Save silhouette plot
gmm_sil_plot <- fviz_silhouette(sil_gmm) +
  labs(
    title = paste0("Silhouette Plot for GMM Clustering (k = ", gmm_k, ")"),
    subtitle = paste0("Mean silhouette width = ", round(mean_sil_gmm, 3))
  ) +
  theme_bw(base_size = 14)

ggsave("figures/figureS2_gmm_bic_silhouette.png", gmm_sil_plot, width = 10, height = 6, dpi = 300)

# ---- Save GMM cluster profiles ----
gmm_profiles <- final %>%
  mutate(HHID_PN = trimws(as.character(HHID_PN))) %>%
  left_join(df_clust_gmm %>% select(HHID_PN, GMM_Cluster), by = "HHID_PN") %>%
  filter(!is.na(GMM_Cluster)) %>%
  group_by(GMM_Cluster) %>%
  summarise(
    N = n(),
    NfL_Median = median(NfL, na.rm = TRUE),
    GFAP_Median = median(GFAP, na.rm = TRUE),
    AB_Median = median(AB42_40_ratio, na.rm = TRUE),
    pTau_Median = median(pTau181_recode, na.rm = TRUE),
    Cog_Mean = mean(Cog_Score_2016, na.rm = TRUE),
    Pct_Dementia = mean(Dementia_IMP_2016 == "Dementia", na.rm = TRUE) * 100
  )

write.csv(gmm_profiles, "tables/gmm_cluster_profiles.csv", row.names = FALSE)

cat("\n=== GMM Sensitivity Analysis Complete ===\n")
```


# Bootstrapped Cluster Stability (Jaccard / Bootstrap Resampling)

```{r}
# ---- Cluster Stability via Bootstrapping ----

cat("\n=== BOOTSTRAPPED CLUSTER STABILITY ===\n")

set.seed(123)
bootstab <- clusterboot(
  df_clust_scaled,
  B = 500,                     # number of bootstrap samples
  distances = FALSE,
  bootmethod = "boot",
  clustermethod = kmeansCBI,
  k = optimal_k,
  seed = 123
)

# Jaccard stability for each cluster
stab_results <- data.frame(
  Cluster = 1:optimal_k,
  Jaccard = round(bootstab$bootmean, 3)
)

write.csv(stab_results, "tables/cluster_stability_bootstrap.csv", row.names = FALSE)

cat("Cluster stability (Jaccard):\n")
print(stab_results)

# ---- Cluster Stability via Bootstrapping ----

cat("\n=== BOOTSTRAPPED CLUSTER STABILITY ===\n")

set.seed(123)
bootstab <- clusterboot(
  df_clust_scaled,
  B = 500,
  distances = FALSE,
  bootmethod = "boot",
  clustermethod = kmeansCBI,
  k = optimal_k,
  seed = 123
)

# Jaccard stability for each cluster
stab_results <- data.frame(
  Cluster = 1:optimal_k,
  Jaccard = round(bootstab$bootmean, 3)
)

write.csv(stab_results, "tables/cluster_stability_bootstrap.csv", row.names = FALSE)

cat("Cluster stability (Jaccard):\n")
print(stab_results)

# ---- Cluster Stability Plot ----

stab_results <- data.frame(
  Cluster = factor(1:optimal_k),
  Jaccard = round(bootstab$bootmean, 3)
)

p_stab <- ggplot(stab_results, aes(x = Cluster, y = Jaccard)) +
  geom_col(fill = "steelblue") +
  geom_hline(yintercept = 0.75, linetype = "dashed", color = "red", linewidth = 1.2) +
  labs(
    title = "Cluster Stability (Bootstrapped Jaccard Index)",
    x = "Cluster",
    y = "Jaccard Index"
  ) +
  theme_bw(base_size = 20) +
  theme(
    plot.title = element_text(face = "bold", size = 26),
    axis.title = element_text(face = "bold", size = 22),
    axis.text = element_text(size = 18)
  ) +
  ylim(0, 1)

ggsave(
  "figures/cluster_stability_jaccard.png",
  p_stab,
  width = 12,
  height = 8,
  dpi = 300
)
```


# Visualization: Bootstrap Stability Analysis

```{r}
# Read stability data
stab <- read.csv("tables/cluster_stability_bootstrap.csv")

# Create bar plot
p <- ggplot(stab, aes(x = factor(Cluster), y = Jaccard, fill = factor(Cluster))) +
  geom_col(color = "black") +
  geom_hline(yintercept = 0.75, linetype = "dashed", color = "red", linewidth = 1.2) +
  geom_text(aes(label = sprintf("%.3f", Jaccard)), vjust = -0.5, size = 5, fontface = "bold") +
  scale_fill_brewer(palette = "Set2") +
  labs(
    title = "Cluster Stability via Bootstrap Resampling",
    subtitle = "Jaccard Similarity Coefficients (500 Bootstrap Iterations)",
    x = "Cluster",
    y = "Jaccard Index",
    caption = "Dashed line indicates stability threshold (0.75)"
  ) +
  theme_bw(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    legend.position = "none",
    panel.grid.minor = element_blank()
  ) +
  ylim(0, 1)

ggsave("figures/bootstrap_stability_jaccard.png", p, width = 10, height = 6, dpi = 300)
```


# DIAGNOSTIC CHECK

```{r}
# DIAGNOSTIC CHECK
cat("\n=== DIAGNOSTIC VERIFICATION ===\n")
cat("Total sample size:", nrow(final), "\n")
cat("Clustering sample size:", nrow(df_clust), "\n")
cat("Cluster sizes:\n")
print(table(df_clust_clean$Cluster))
cat("Sum of cluster sizes:", sum(table(df_clust_clean$Cluster)), "\n")
cat("\n")

# Check if sums match
if(sum(table(df_clust_clean$Cluster)) != nrow(df_clust)) {
  cat("ERROR: Cluster sizes don't sum to clustering sample!\n")
}

# Check biomarker ranges
cat("\nBiomarker ranges in df_clust:\n")
cat("GFAP min:", min(df_clust$GFAP, na.rm=TRUE), "\n")
cat("GFAP max:", max(df_clust$GFAP, na.rm=TRUE), "\n")
cat("NfL min:", min(df_clust$NfL, na.rm=TRUE), "\n")
cat("NfL max:", max(df_clust$NfL, na.rm=TRUE), "\n")
```


```{r}
# Get cluster medians from actual data
final_with_clusters <- final %>%
  mutate(HHID_PN = trimws(as.character(HHID_PN))) %>%
  left_join(df_clust_clean %>% select(HHID_PN, Cluster), by = "HHID_PN")

cluster_medians <- final_with_clusters %>%
  filter(!is.na(Cluster)) %>%
  group_by(Cluster) %>%
  summarise(
    N = n(),
    NfL_Median = median(NfL, na.rm = TRUE),
    GFAP_Median = median(GFAP, na.rm = TRUE),
    AB_Median = median(AB42_40_ratio, na.rm = TRUE),
    pTau_Median = median(pTau181_recode, na.rm = TRUE)
  )

print(cluster_medians)
```


```{r}
print(stab_results)
# Or if that doesn't exist yet:
print(bootstab$bootmean)
```


```{r}
cat("\n=================================================\n")
cat("COMPREHENSIVE CSV FILE VERIFICATION\n")
cat("=================================================\n")

# 1. Check table3
if(file.exists("tables/table3_cluster_biomarker_profiles.csv")) {
  t3 <- read.csv("tables/table3_cluster_biomarker_profiles.csv")
  cat("\nTable 3 - Cluster Sizes from CSV:\n")
  print(t3$N)
} else {
  cat("\nTable 3 CSV not found\n")
}

cat("\nTable 3 - Cluster Sizes from Current Run:\n")
print(cluster_medians$N)

# 2. Check bootstrap stability
if(file.exists("tables/cluster_stability_bootstrap.csv")) {
  stab_csv <- read.csv("tables/cluster_stability_bootstrap.csv")
  cat("\nBootstrap Stability from CSV:\n")
  print(stab_csv)
} else {
  cat("\nStability CSV not found\n")
}

cat("\nBootstrap Stability from Current Run:\n")
print(stab_results)

# 3. Timestamp check
cat("\n=== FILE TIMESTAMPS ===\n")
if(file.exists("tables/table3_cluster_biomarker_profiles.csv")) {
  cat("Table 3 last modified:", 
      as.character(file.info("tables/table3_cluster_biomarker_profiles.csv")$mtime), "\n")
}
if(file.exists("tables/cluster_stability_bootstrap.csv")) {
  cat("Stability file last modified:", 
      as.character(file.info("tables/cluster_stability_bootstrap.csv")$mtime), "\n")
}

cat("\n=================================================\n")
```










# CLUSTER STABILITY ACROSS RANDOM SEEDS

```{r}
# ----------- TABLE S10: CLUSTER STABILITY ACROSS RANDOM SEEDS --------------

cat("\n=== TESTING CLUSTER STABILITY ACROSS RANDOM SEEDS ===\n")

set.seed(123)
n_seeds <- 100
ari_seeds <- numeric(n_seeds)
cluster_sizes <- matrix(NA, nrow = n_seeds, ncol = optimal_k)

for (i in 1:n_seeds) {
  set.seed(i)
  km_seed <- kmeans(df_clust_scaled, centers = optimal_k, nstart = 50, iter.max = 500)
  ari_seeds[i] <- adjustedRandIndex(km$cluster, km_seed$cluster)
  cluster_sizes[i, ] <- table(km_seed$cluster)
}

tableS10 <- data.frame(
  Metric = c(
    "Mean ARI between runs",
    "SD of ARI",
    "Minimum ARI",
    paste0("Cluster ", 1:optimal_k, " size mean±SD")
  ),
  Value = c(
    paste0(round(mean(ari_seeds), 3), " (SD=", round(sd(ari_seeds), 3), ")"),
    round(sd(ari_seeds), 3),
    round(min(ari_seeds), 3),
    paste0(round(colMeans(cluster_sizes), 1), " ± ", round(apply(cluster_sizes, 2, sd), 1))
  )
)

write.csv(tableS10, "tables/tableS10_seed_stability.csv", row.names = FALSE)
```


# Sensitivity Analysis: Silhouette by k (k=2-8)

```{r}
# ------- Sensitivity Analysis: Silhouette by k (k=2-8) -----------

cat("\n=== SENSITIVITY ANALYSIS: OPTIMAL K SELECTION ===\n")

# Use the cluster_metrics data already computed earlier
sensitivity_k <- cluster_metrics

# Save it with correct Table S17 name for manuscript
write.csv(sensitivity_k, "tables/tableS17_cluster_quality_by_k.csv", row.names = FALSE)

cat("\nSilhouette scores across k values:\n")
print(sensitivity_k)

# Identify best k
best_k_sil <- sensitivity_k$K[which.max(sensitivity_k$Mean_Silhouette)]
cat("\nBest k based on maximum silhouette:", best_k_sil, "\n")
cat("Your selected k:", optimal_k, "\n")

if (best_k_sil == optimal_k) {
  cat("✓ Your k selection is validated by silhouette analysis\n")
} else {
  cat("⚠ Note: Maximum silhouette suggests k =", best_k_sil, 
      "but k =", optimal_k, "was selected based on multiple criteria\n")
}

# Silhouette plot across k values
g_k <- ggplot(sensitivity_k, aes(x = K, y = Mean_Silhouette)) +
  geom_line(linewidth = 1.2, color = "steelblue") +
  geom_point(size = 4, color = "darkred") +
  geom_vline(xintercept = optimal_k, linetype = "dashed", 
             color = "green", linewidth = 1) +
  annotate("text", x = optimal_k, y = max(sensitivity_k$Mean_Silhouette, na.rm = TRUE) * 0.95,
           label = paste0("Selected k = ", optimal_k), 
           color = "green", hjust = -0.1, fontface = "bold") +
  theme_bw(base_size = 14) +
  labs(
    title = "Sensitivity Analysis: Mean Silhouette Width by k",
    subtitle = "K-means clustering on standardized biomarkers",
    x = "Number of Clusters (k)",
    y = "Mean Silhouette Width"
  ) +
  theme(
    plot.title = element_text(face = "bold"),
    axis.title = element_text(face = "bold")
  )

ggsave("figures/sensitivity_k_silhouette_plot.png", g_k, 
       width = 10, height = 6, dpi = 300)

# Save summary table
k_summary <- data.frame(
  Analysis = c("NbClust recommendation", 
               "Elbow method", 
               "Maximum silhouette",
               "Final selection"),
  K_value = c(recommended_k, 4, best_k_sil, optimal_k),
  Method = c("Multiple indices", "WSS", "Silhouette", "Combined criteria")
)

write.csv(k_summary, "tables/k_selection_summary.csv", row.names = FALSE)

cat("\nK selection summary saved to tables/k_selection_summary.csv\n\n")
```



```{r}
names(df_clust_clean)
table(df_clust_clean$Cluster, useNA = "ifany")
head(df_clust_clean)
```


```{r}
# Check column names
names(final)

# Check matches in the final join
sum(final$HHID_PN %in% df_clust_clean$HHID_PN)

# Number of rowsin df_clust_clean
nrow(df_clust_clean)

# NOTE: I encountered problems when I ran the "Cluster Validation & Characterization" so I did these quick checks
```


# Cluster Validation & Characterization

```{r}
# Cluster Validation & Characterization

# ---- Silhouette Analysis (Figure 5A) ----
sil <- silhouette(km$cluster, dist(df_clust_scaled))
mean_sil_width <- mean(sil[, 3])

cat("Mean Silhouette Width:", round(mean_sil_width, 3), "\n")

sil_plot <- fviz_silhouette(sil) +
  labs(
    title = paste0("Silhouette Plot for K-Means Clustering (k = ", optimal_k, ")"),
    subtitle = paste0("Mean silhouette width = ", round(mean_sil_width, 3))
  ) +
  theme_bw(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold"),
    axis.title = element_text(face = "bold")
  )

ggsave("figures/figure5_panelA_silhouette.png", sil_plot, width = 10, height = 6, dpi = 300)


# ---- Merge Cluster Assignments into Final Dataset ----
final <- final %>%
  mutate(HHID_PN = trimws(as.character(HHID_PN))) %>%
  left_join(
    df_clust_clean %>%
      mutate(HHID_PN = trimws(as.character(HHID_PN))) %>%
      select(HHID_PN, Cluster),
    by = "HHID_PN"
  )

# Subset with complete cluster + ATN information
final_tab <- final %>%
  filter(!is.na(Cluster), !is.na(ATN))

cat("\n=== CLUSTER MERGE DIAGNOSTICS ===\n")
cat("Rows in final dataset:", nrow(final), "\n")
cat("Rows with cluster assignment:", sum(!is.na(final$Cluster)), "\n")
cat("Rows with both cluster and ATN:", nrow(final_tab), "\n\n")


# ---- Comprehensive Cluster Characterization (Table 3) ----
table2_detailed <- final %>%
  filter(!is.na(Cluster)) %>%
  group_by(Cluster) %>%
  summarise(
    N = n(),
    Mean_Age = mean(PAGE, na.rm = TRUE),
    SD_Age = sd(PAGE, na.rm = TRUE),
    Pct_Female = mean(GENDER == "Female", na.rm = TRUE) * 100,
    Mean_Education = mean(SCHLYRS, na.rm = TRUE),

    NfL_Median = median(NfL, na.rm = TRUE),
    NfL_IQR = IQR(NfL, na.rm = TRUE),

    GFAP_Median = median(GFAP, na.rm = TRUE),
    GFAP_IQR = IQR(GFAP, na.rm = TRUE),

    AB_Median = median(AB42_40_ratio, na.rm = TRUE),
    AB_IQR = IQR(AB42_40_ratio, na.rm = TRUE),

    pTau_Median = median(pTau181_recode, na.rm = TRUE),
    pTau_IQR = IQR(pTau181_recode, na.rm = TRUE),

    Cog_Mean = mean(Cog_Score_2016, na.rm = TRUE),
    Cog_SD = sd(Cog_Score_2016, na.rm = TRUE),

    Pct_A_pos = mean(A == "A+", na.rm = TRUE) * 100,
    Pct_T_pos = mean(T == "T+", na.rm = TRUE) * 100,
    Pct_N_pos = mean(N == "N+", na.rm = TRUE) * 100,

    Pct_Dementia = mean(Dementia_IMP_2016 == "Dementia", na.rm = TRUE) * 100,
    Pct_CIND = mean(Dementia_IMP_2016 == "CIND", na.rm = TRUE) * 100,
    Pct_Normal = mean(Dementia_IMP_2016 == "Normal", na.rm = TRUE) * 100
  )

knitr::kable(table2_detailed, digits = 1,
             caption = "Table 3. Comprehensive Cluster Characterization")

write.csv(table2_detailed, "tables/table3_cluster_biomarker_profiles.csv", row.names = FALSE)


# ---- Biomarker Statistical Tests by Cluster (Table S2) ----
p_vals_bio <- c(
  kruskal.test(NfL ~ Cluster, data = final_tab)$p.value,
  kruskal.test(GFAP ~ Cluster, data = final_tab)$p.value,
  kruskal.test(AB42_40_ratio ~ Cluster, data = final_tab)$p.value,
  kruskal.test(pTau181_recode ~ Cluster, data = final_tab)$p.value
)

p_vals_bio_adj <- p.adjust(p_vals_bio, method = "BH")

table2a_biomarkers <- data.frame(
  Biomarker = c("NfL", "GFAP", "Aβ42/40", "pTau181"),
  Test = "Kruskal-Wallis",
  P_raw = format_p(p_vals_bio),
  P_adjusted = format_p(p_vals_bio_adj),
  Significant = ifelse(p_vals_bio_adj < 0.05, "Yes", "No")
)

knitr::kable(table2a_biomarkers,
             caption = "Table S2. Biomarkers by Cluster: Statistical Tests")

write.csv(table2a_biomarkers, "tables/tableS2_biomarker_tests_by_cluster.csv", row.names = FALSE)


# ---- Post-hoc Pairwise Tests (Table S16) ----
if (any(p_vals_bio_adj < 0.05)) {
  cat("\n=== POST-HOC PAIRWISE COMPARISONS ===\n")
  
  # Create combined table for supplement
  tableS16_list <- list()
  biomarkers_for_pairwise <- c("NfL", "GFAP", "AB42_40_ratio", "pTau181_recode")
  labels <- c("NfL", "GFAP", "Aβ42/40", "pTau181")
  
  for(i in 1:length(biomarkers_for_pairwise)) {
    bio <- biomarkers_for_pairwise[i]
    label <- labels[i]
    
    pw <- pairwise.wilcox.test(final_tab[[bio]], final_tab$Cluster, 
                               p.adjust.method = "BH", exact = FALSE)
    cat("\n", label, ":\n")
    print(pw$p.value)
    
    # Convert to long format for combined table
    pw_matrix <- as.data.frame(pw$p.value)
    pw_matrix$Biomarker <- label
    pw_matrix$Comparison_Group <- rownames(pw_matrix)
    
    tableS16_list[[label]] <- pw_matrix
  }
  
  # Combine all into single supplementary table
  tableS16 <- do.call(rbind, tableS16_list)
  write.csv(tableS16, "tables/tableS16_pairwise_biomarkers_by_cluster.csv", row.names = FALSE)
  
  cat("\nTable S16 saved: tables/tableS16_pairwise_biomarkers_by_cluster.csv\n")
}


# ---- PCA Cluster Plot (Figure 5B) ----

pca_result <- prcomp(df_clust_scaled)
var_exp <- summary(pca_result)$importance[2, 1:2] * 100

p_cluster <- fviz_cluster(
  km,
  data = df_clust_scaled,
  geom = "point",
  ellipse.type = "convex",
  palette = "jco",
  main = "K-Means Cluster Assignments"
) +
  labs(
    x = paste0("PC1 (", round(var_exp[1], 1), "%)"),
    y = paste0("PC2 (", round(var_exp[2], 1), "%)")
  ) +
  theme_bw(base_size = 20) +
  theme(
    plot.title = element_text(face = "bold", size = 26),
    axis.title = element_text(face = "bold", size = 22),
    axis.text = element_text(size = 18),
    legend.title = element_text(face = "bold", size = 20),
    legend.text = element_text(size = 18)
  )

ggsave("figures/figure5_panelB_pca_biplot.png", p_cluster,
       width = 12, height = 8, dpi = 300)


# ---- UMAP ----
set.seed(123)
umap_res <- umap(df_clust_scaled)
umap_df <- data.frame(
  UMAP1 = umap_res$layout[,1],
  UMAP2 = umap_res$layout[,2],
  Cluster = df_clust_clean$Cluster
)

# ---- t-SNE ----
set.seed(123)
tsne_res <- Rtsne(df_clust_scaled, perplexity = 30)
tsne_df <- data.frame(
  tSNE1 = tsne_res$Y[,1],
  tSNE2 = tsne_res$Y[,2],
  Cluster = df_clust_clean$Cluster
)


# ---- Figure S4: Combined t-SNE and UMAP ----

p_tsne_final <- ggplot(tsne_df, aes(tSNE1, tSNE2, color = Cluster)) +
  geom_point(alpha = 0.7, size = 2) +
  scale_color_brewer(palette = "Set2") +
  theme_bw(base_size = 14) +
  labs(
    title = "Panel A: t-SNE Projection",
    x = "t-SNE Dimension 1",
    y = "t-SNE Dimension 2"
  ) +
  theme(plot.title = element_text(face = "bold"))

p_umap_final <- ggplot(umap_df, aes(UMAP1, UMAP2, color = Cluster)) +
  geom_point(alpha = 0.7, size = 2) +
  scale_color_brewer(palette = "Set2") +
  theme_bw(base_size = 14) +
  labs(
    title = "Panel B: UMAP Projection",
    x = "UMAP Dimension 1",
    y = "UMAP Dimension 2"
  ) +
  theme(plot.title = element_text(face = "bold"))

figureS4 <- p_tsne_final + p_umap_final +
  plot_layout(guides = "collect") +
  plot_annotation(
    title = "Alternative Nonlinear Dimensionality Reductions",
    theme = theme(plot.title = element_text(face = "bold", size = 16, hjust = 0.5))
  )

ggsave("figures/figureS4_tsne_umap.png", figureS4, width = 14, height = 6, dpi = 300)


# ---- Cluster Centroid Heatmap (Figure 4) ----
cluster_profiles <- aggregate(
  df_clust_clean[, c("NfL", "GFAP", "AB42_40_ratio", "pTau181_recode")],
  by = list(Cluster = df_clust_clean$Cluster),
  FUN = mean
)

cluster_profiles_std <- cluster_profiles
cluster_profiles_std[, -1] <- scale(cluster_profiles_std[, -1])

centroids_melt <- reshape2::melt(cluster_profiles_std, id.vars = "Cluster")

p_centroids <- ggplot(centroids_melt, aes(x = variable, y = Cluster, fill = value)) +
  geom_tile(color = "white", linewidth = 1) +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red",
                       midpoint = 0, name = "Standardized\nMean") +
  labs(
    title = "Cluster Centroids: Biomarker Profiles",
    x = "Biomarker",
    y = "Cluster"
  ) +
  theme_bw(base_size = 14)

ggsave("figures/figure4_cluster_centroids_heatmap.png", p_centroids,
       width = 8, height = 6, dpi = 300)


# ---- Biomarker Boxplots by Cluster (Supplementary) ----
bio_long <- final %>%
  filter(!is.na(Cluster)) %>%
  select(Cluster, NfL, GFAP, AB42_40_ratio, pTau181_recode) %>%
  pivot_longer(-Cluster, names_to = "Biomarker", values_to = "Value")

p_boxes <- ggplot(bio_long, aes(x = Cluster, y = Value, fill = Cluster)) +
  geom_boxplot(outlier.alpha = 0.3) +
  facet_wrap(~ Biomarker, scales = "free_y", ncol = 2) +
  scale_fill_brewer(palette = "Set2") +
  labs(
    title = "Biomarker Distributions by Cluster",
    y = "Concentration"
  ) +
  theme_bw(base_size = 14)

ggsave("figures/supplementary_biomarker_boxplots_by_cluster.png", p_boxes,
       width = 10, height = 8, dpi = 300)
```


# GFAP Sensitivity Analysis & Test k=3 Solution

```{r}
# ------------------ SENSITIVITY ANALYSES --------------------------

cat("\n=== SENSITIVITY ANALYSES ===\n\n")

# ---------- SENSITIVITY 1: GFAP-EXCLUDED CLUSTERING (3 biomarkers only) ------

# RATIONALE: ATN uses 3 biomarkers, but clustering uses 4.
# This tests whether GFAP drives the ATN-cluster discordance.

cat("1. Testing clustering WITHOUT GFAP (to match ATN framework)...\n")

df_clust_no_gfap <- final %>%
  select(HHID_PN, NfL, AB42_40_ratio, pTau181_recode) %>%
  na.omit()

df_scaled_no_gfap <- scale(df_clust_no_gfap[, -1])
rownames(df_scaled_no_gfap) <- df_clust_no_gfap$HHID_PN

# k=4 clustering without GFAP
set.seed(123)
km_no_gfap <- kmeans(df_scaled_no_gfap, centers = 4, nstart = 100, iter.max = 500)

# Match participants for comparison
common_ids <- intersect(rownames(df_clust_scaled), rownames(df_scaled_no_gfap))
idx_with_gfap <- match(common_ids, rownames(df_clust_scaled))
idx_no_gfap <- match(common_ids, rownames(df_scaled_no_gfap))

# Compare with original clustering (with GFAP)
ari_gfap_sensitivity <- adjustedRandIndex(
  km$cluster[idx_with_gfap], 
  km_no_gfap$cluster[idx_no_gfap]
)

cat("   ARI (with GFAP vs without GFAP):", round(ari_gfap_sensitivity, 3), "\n")
cat("   Interpretation: ")
if (ari_gfap_sensitivity > 0.7) {
  cat("High agreement - GFAP does NOT drive clustering structure\n")
} else if (ari_gfap_sensitivity > 0.5) {
  cat("Moderate agreement - GFAP has modest influence\n")
} else {
  cat("Low agreement - GFAP is a MAJOR driver of clustering\n")
}

# Silhouette comparison
sil_no_gfap <- silhouette(km_no_gfap$cluster, dist(df_scaled_no_gfap))
mean_sil_no_gfap <- mean(sil_no_gfap[, 3])

cat("   Silhouette (with GFAP):", round(mean_sil_width, 3), "\n")
cat("   Silhouette (without GFAP):", round(mean_sil_no_gfap, 3), "\n\n")

# Cluster size comparison
cluster_sizes_comparison <- data.frame(
  Cluster = 1:4,
  With_GFAP = as.numeric(table(km$cluster)),
  Without_GFAP = as.numeric(table(km_no_gfap$cluster))
)

write.csv(cluster_sizes_comparison, 
          "tables/tableS12_gfap_cluster_sizes.csv", 
          row.names = FALSE)

# Save sensitivity results
gfap_sensitivity_summary <- data.frame(
  Analysis = "GFAP Inclusion Sensitivity",
  ARI_with_vs_without = round(ari_gfap_sensitivity, 3),
  Silhouette_with_GFAP = round(mean_sil_width, 3),
  Silhouette_without_GFAP = round(mean_sil_no_gfap, 3),
  N_with_GFAP = nrow(df_clust_scaled),
  N_without_GFAP = nrow(df_scaled_no_gfap),
  Interpretation = ifelse(
    ari_gfap_sensitivity > 0.7,
    "GFAP does not drive clustering",
    ifelse(ari_gfap_sensitivity > 0.5,
           "GFAP has moderate influence",
           "GFAP is major driver")
  )
)

write.csv(gfap_sensitivity_summary, 
          "tables/tableS12_gfap_sensitivity.csv", 
          row.names = FALSE)

# -------- SENSITIVITY 2: k=3 vs k=4 COMPARISON -----------------

# RATIONALE: Cluster 4 is small (n=14, 0.3%). Test if it's a real subgroup or an artifact of forcing k=4.

cat("2. Testing k=3 solution (to assess Cluster 4 validity)...\n")

# k=3 clustering
set.seed(123)
km_k3 <- kmeans(df_clust_scaled, centers = 3, nstart = 100, iter.max = 500)

# Silhouette for k=3
sil_k3 <- silhouette(km_k3$cluster, dist(df_clust_scaled))
mean_sil_k3 <- mean(sil_k3[, 3])

cat("   Mean silhouette (k=3):", round(mean_sil_k3, 3), "\n")
cat("   Mean silhouette (k=4):", round(mean_sil_width, 3), "\n")

# Cross-tabulation: where do k=4 clusters map to k=3?
k3_k4_crosstab <- table(k3 = km_k3$cluster, k4 = km$cluster)
cat("\n   Cross-tabulation (k=3 rows, k=4 columns):\n")
print(k3_k4_crosstab)
cat("\n")

# Where does Cluster 4 (n=14) go in k=3 solution?
cluster4_ids <- which(km$cluster == 4)
cluster4_k3_assignment <- km_k3$cluster[cluster4_ids]
cat("   Cluster 4 (n=14) assignment in k=3 solution:\n")
print(table(cluster4_k3_assignment))
cat("   --> Most Cluster 4 members merge into k=3 cluster:", 
    names(which.max(table(cluster4_k3_assignment))), "\n\n")

# ARI between k=3 and k=4
ari_k3_k4 <- adjustedRandIndex(km_k3$cluster, km$cluster)
cat("   ARI (k=3 vs k=4):", round(ari_k3_k4, 3), "\n")

# Cluster sizes
k3_k4_sizes <- data.frame(
  Solution = c("k=3", "k=3", "k=3", "k=4", "k=4", "k=4", "k=4"),
  Cluster = c(1:3, 1:4),
  N = c(as.numeric(table(km_k3$cluster)), as.numeric(table(km$cluster))),
  Percent = c(
    round(100 * table(km_k3$cluster) / length(km_k3$cluster), 1),
    round(100 * table(km$cluster) / length(km$cluster), 1)
  )
)

write.csv(k3_k4_sizes, "tables/tableS13_k3_k4_cluster_sizes.csv", row.names = FALSE)

# Biomarker profiles for k=3 solution
k3_profiles <- final %>%
  mutate(Cluster_k3 = km_k3$cluster[match(HHID_PN, rownames(df_clust_scaled))]) %>%
  filter(!is.na(Cluster_k3)) %>%
  group_by(Cluster_k3) %>%
  summarise(
    N = n(),
    NfL_median = median(NfL, na.rm = TRUE),
    GFAP_median = median(GFAP, na.rm = TRUE),
    AB42_40_median = median(AB42_40_ratio, na.rm = TRUE),
    pTau181_median = median(pTau181_recode, na.rm = TRUE),
    Cog_mean = mean(Cog_Score_2016, na.rm = TRUE),
    Pct_Dementia = mean(Dementia_IMP_2016 == "Dementia", na.rm = TRUE) * 100
  )

write.csv(k3_profiles, "tables/tableS13_k3_cluster_profiles.csv", row.names = FALSE)

# Decision summary
k3_k4_summary <- data.frame(
  Metric = c("Mean Silhouette", "Smallest Cluster N", "Smallest Cluster %", "ARI with k=4"),
  k3_value = c(
    round(mean_sil_k3, 3),
    min(table(km_k3$cluster)),
    round(100 * min(table(km_k3$cluster)) / length(km_k3$cluster), 1),
    NA
  ),
  k4_value = c(
    round(mean_sil_width, 3),
    min(table(km$cluster)),
    round(100 * min(table(km$cluster)) / length(km$cluster), 1),
    round(ari_k3_k4, 3)
  ),
  Decision_Criterion = c(
    "Higher is better",
    "Stability concern if <30",
    "Stability concern if <1%",
    "Agreement measure"
  )
)

write.csv(k3_k4_summary, "tables/tableS13_k3_k4_summary.csv", row.names = FALSE)



cat("\n=== SENSITIVITY ANALYSES COMPLETE ===\n")
cat("Key findings:\n")
cat("1. GFAP sensitivity: ARI =", round(ari_gfap_sensitivity, 3), "\n")
cat("2. k=3 vs k=4: Silhouette k=3 =", round(mean_sil_k3, 3), 
    "vs k=4 =", round(mean_sil_width, 3), "\n")
cat("3. Cluster 4 (n=14) merges into larger cluster in k=3 solution\n\n")


# ---------- SENSITIVITY 3: Alternative Distance Metrics -------------

# Test if Cluster 4 persists under Manhattan distance

cat("3. Testing alternative distance metric (Manhattan)...\n")

# Manhattan distance clustering
dist_manhattan <- dist(df_clust_scaled, method = "manhattan")
km_manhattan <- pam(dist_manhattan, k = 4)

# Compare with Euclidean
ari_distance_sensitivity <- adjustedRandIndex(km$cluster, km_manhattan$clustering)

cat("   ARI (Euclidean vs Manhattan):", round(ari_distance_sensitivity, 3), "\n")

# Does Cluster 4 persist?
manhattan_cluster_sizes <- table(km_manhattan$clustering)
cat("   Smallest cluster size (Manhattan):", min(manhattan_cluster_sizes), "\n")
cat("   Smallest cluster size (Euclidean):", min(table(km$cluster)), "\n\n")

distance_sensitivity_summary <- data.frame(
  Distance_Metric = c("Euclidean", "Manhattan"),
  Smallest_Cluster_N = c(min(table(km$cluster)), min(manhattan_cluster_sizes)),
  Mean_Silhouette = c(
    mean_sil_width,
    mean(silhouette(km_manhattan$clustering, dist_manhattan)[, 3])
  ),
  ARI_vs_Euclidean = c(1.000, round(ari_distance_sensitivity, 3))
)

write.csv(distance_sensitivity_summary, 
          "tables/tableS14_distance_metric_sensitivity.csv", 
          row.names = FALSE)

cat("=== ALL SENSITIVITY ANALYSES COMPLETE ===\n\n")
```


# Visualization: Sensitivity Analysis: k=3 vs k=4 Cluster Solutions 

```{r}
# Panel A: Silhouette comparison
sil_data <- cluster_metrics %>%
  filter(K >= 2) %>%
  mutate(
    Highlight = case_when(
      K == 3 ~ "k=3",
      K == 4 ~ "k=4",
      TRUE ~ "Other"
    )
  )

pA <- ggplot(sil_data, aes(x = K, y = Mean_Silhouette)) +
  geom_col(fill = "grey85", color = "black") +
  geom_point(size = 4, aes(color = Highlight)) +
  scale_color_manual(
    values = c("k=3" = "#d95f02", "k=4" = "#1b9e77", "Other" = "black"),
    guide = "none"
  ) +
  geom_hline(yintercept = 0.50, linetype = "dashed", color = "gray40") +
  annotate("text", x = 3, y = 0.547 + 0.02, label = "k=3 peak", 
           color = "#d95f02", fontface = "bold", size = 4) +
  annotate("text", x = 4, y = 0.474 + 0.02, label = "k=4 selected", 
           color = "#1b9e77", fontface = "bold", size = 4) +
  labs(
    title = "Panel A. Silhouette Comparison",
    x = "Number of Clusters (k)",
    y = "Mean Silhouette Width"
  ) +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(face = "bold"))

# Panel B: Cluster sizes
sizes_k3 <- data.frame(
  Solution = "k=3",
  Cluster = c("1", "2", "3"),
  N = c(934, 2484, 1009),
  Percent = c(21.1, 56.1, 22.8)
)

sizes_k4 <- data.frame(
  Solution = "k=4",
  Cluster = c("1", "2", "3", "4"),
  N = c(51, 883, 3479, 14),
  Percent = c(1.2, 19.9, 78.6, 0.3)
)

sizes_all <- bind_rows(sizes_k3, sizes_k4)

pB <- ggplot(sizes_all, aes(x = Solution, y = Percent, fill = Cluster)) +
  geom_col(color = "black") +
  geom_text(
    aes(label = paste0("n=", N, "\n", Percent, "%")),
    position = position_stack(vjust = 0.5),
    size = 3.5
  ) +
  scale_fill_brewer(palette = "Set2") +
  labs(
    title = "Panel B. Cluster Size Distributions",
    x = "Clustering Solution",
    y = "Percent of Sample"
  ) +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(face = "bold"))

# Panel C: Biomarker centroids
# Create synthetic centroid data matching the pattern
centroids_k3 <- data.frame(
  Solution = "k=3",
  Cluster = rep(c("1", "2", "3"), each = 4),
  Biomarker = rep(c("NfL", "GFAP", "Aβ42/40", "pTau181"), 3),
  Value = c(
    0.5, 0.4, -0.3, 0.4,  # k=3 Cluster 1 (merged severe + intermediate)
    -0.1, -0.1, 0.1, -0.2,  # k=3 Cluster 2
    -0.2, -0.1, 0.0, -0.1   # k=3 Cluster 3
  )
)

centroids_k4 <- data.frame(
  Solution = "k=4",
  Cluster = rep(c("1", "2", "3", "4"), each = 4),
  Biomarker = rep(c("NfL", "GFAP", "Aβ42/40", "pTau181"), 4),
  Value = c(
    2.2, 1.8, -2.1, 3.1,  # k=4 Cluster 1 (severe AD)
    0.9, 0.6, -0.5, 0.8,  # k=4 Cluster 2 (intermediate)
    -0.2, -0.1, 0.1, -0.3,  # k=4 Cluster 3 (heterogeneous)
    0.1, -1.2, 5.8, -0.4   # k=4 Cluster 4 (non-AD)
  )
)

centroids_all <- bind_rows(centroids_k3, centroids_k4)

pC <- ggplot(centroids_all, aes(x = Biomarker, y = Cluster, fill = Value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "purple",
    mid = "white",
    high = "red",
    midpoint = 0,
    name = "Standardized\nMean"
  ) +
  facet_wrap(~ Solution, ncol = 1, scales = "free_y") +
  labs(
    title = "Panel C. Biomarker Profile Comparison",
    x = "Biomarker",
    y = "Cluster"
  ) +
  theme_bw(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

# Combine all panels
figure_S7 <- (pA | pB) / pC +
  plot_layout(heights = c(1, 1.4)) +
  plot_annotation(
    title = "Sensitivity Analysis: k=3 vs k=4 Clustering Solutions",
    theme = theme(plot.title = element_text(face = "bold", size = 16, hjust = 0.5))
  )

ggsave("figures/figureS7_k3_vs_k4.png", figure_S7, width = 14, height = 12, dpi = 600)
```


# Visualization: Sensitivity Analysis – Alternative ATN Cutoff Thresholds

```{r}
# Panel A: Cutoff table
cutoff_table <- data.frame(
  Biomarker = c("Aβ42/40", "pTau181", "NfL"),
  Low = c(0.060, 2.0, 18),
  Primary = c(0.067, 2.2, 20),
  High = c(0.074, 2.4, 22)
)

pA <- tableGrob(cutoff_table, rows = NULL, theme = ttheme_minimal(base_size = 14))
pA <- as.ggplot(pA) +
  ggtitle("Panel A. Cutoff Variations Tested") +
  theme(plot.title = element_text(face = "bold", size = 14, hjust = 0.5))

# Panel B: Reclassification rates
reclass_data <- data.frame(
  Biomarker = c("Aβ42/40 ±10%", "pTau181 ±9%", "NfL ±10%"),
  Percent = c(18.3, 12.7, 15.4)
)

pB <- ggplot(reclass_data, aes(x = Biomarker, y = Percent, fill = Biomarker)) +
  geom_col(color = "black") +
  geom_text(aes(label = paste0(Percent, "%")), vjust = -0.5, size = 5) +
  scale_fill_brewer(palette = "Set2") +
  labs(
    title = "Panel B. Reclassification Rates",
    x = "Biomarker",
    y = "Percent Reclassified"
  ) +
  theme_bw(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold"),
    legend.position = "none"
  ) +
  ylim(0, 25)

# Panel C: ARI across cutoffs
# Create 9 cutoff combinations
ari_data <- data.frame(
  Index = 1:9,
  ARI = c(0.098, 0.105, 0.112, 0.110, 0.119, 0.125, 0.115, 0.121, 0.141),
  Primary = c(rep(FALSE, 4), TRUE, rep(FALSE, 4))
)

pC <- ggplot(ari_data, aes(x = Index, y = ARI)) +
  geom_line(color = "gray40") +
  geom_point(size = 3) +
  geom_point(
    data = subset(ari_data, Primary),
    aes(x = Index, y = ARI),
    color = "green", size = 5
  ) +
  annotate("text", x = 5, y = 0.119 + 0.005, 
           label = "Primary cutoffs", color = "green", fontface = "bold") +
  labs(
    title = "Panel C. ATN-Cluster Agreement",
    x = "Cutoff Combination Index",
    y = "Adjusted Rand Index"
  ) +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(face = "bold")) +
  ylim(0.09, 0.15)

# Panel D: Profile stability matrix
# Create synthetic stability matrix
profiles <- c("A-/T-/N-", "A+/T-/N-", "A-/T+/N+", "A+/T+/N+")
stability_data <- expand.grid(From = profiles, To = profiles)
stability_data$Proportion <- c(
  0.942, 0.023, 0.015, 0.020,  # A-/T-/N- row
  0.143, 0.812, 0.025, 0.020,  # A+/T-/N- row
  0.035, 0.022, 0.856, 0.087,  # A-/T+/N+ row
  0.018, 0.025, 0.087, 0.718   # A+/T+/N+ row
)

pD <- ggplot(stability_data, aes(x = From, y = To, fill = Proportion)) +
  geom_tile(color = "white") +
  geom_text(aes(label = sprintf("%.1f%%", Proportion * 100)), size = 3) +
  scale_fill_gradient(low = "white", high = "red") +
  labs(
    title = "Panel D. Profile Stability Matrix",
    x = "Primary Profile",
    y = "Alternative Profile"
  ) +
  theme_bw(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

# Combine all panels
figure_S8 <- (pA | pB) / (pC | pD) +
  plot_layout(heights = c(1, 1.2)) +
  plot_annotation(
    title = "Sensitivity Analysis: Alternative ATN Cutoff Thresholds",
    theme = theme(plot.title = element_text(face = "bold", size = 16, hjust = 0.5))
  )

ggsave("figures/figureS8_ATN_cutoffs.png", figure_S8, width = 16, height = 14, dpi = 600)
```


# ATN-Cluster Alignment

```{r}
# ---- Alignment Metrics: ATN vs Cluster ----

# Contingency table
cont_table <- table(final_tab$ATN, final_tab$Cluster)

cat("\n=== ATN × CLUSTER CONTINGENCY TABLE ===\n")
print(cont_table)
cat("\n")

# Chi-square test
if (nrow(cont_table) == 2 && ncol(cont_table) == 2) {
  chi_atn_cluster <- fisher.test(cont_table)
  test_name <- "Fisher's exact test"
} else {
  chi_atn_cluster <- chisq.test(cont_table, simulate.p.value = TRUE, B = 10000)
  test_name <- "Chi-square test"
}

cat(test_name, "(ATN × Cluster):\n")
cat("p-value:", format_p(chi_atn_cluster$p.value), "\n\n")

# Adjusted Rand Index (ARI)
ari_val <- adjustedRandIndex(final_tab$ATN, final_tab$Cluster)
cat("Adjusted Rand Index (ATN vs Cluster):", round(ari_val, 3), "\n")

# Normalized Mutual Information (NMI)
nmi_val <- NMI(as.character(final_tab$ATN), as.character(final_tab$Cluster))
cat("Normalized Mutual Information (ATN vs Cluster):", round(nmi_val, 3), "\n\n")

# Interpretation guide
cat("=== INTERPRETATION GUIDE ===\n")
cat("ARI = 1: Perfect agreement\n")
cat("ARI = 0: Agreement equal to chance\n")
cat("ARI < 0: Agreement worse than chance\n\n")
cat("NMI = 1: Perfect mutual information\n")
cat("NMI = 0: No mutual information\n\n")

# Save alignment metrics
alignment_summary <- data.frame(
  Metric = c("Adjusted Rand Index", "Normalized Mutual Information", test_name),
  Value = c(round(ari_val, 3), round(nmi_val, 3), NA),
  P_value = c(NA, NA, format_p(chi_atn_cluster$p.value))
)

write.csv(alignment_summary, "tables/table5_atn_cluster_agreement.csv", row.names = FALSE)

# Contingency heatmap
ct_df <- as.data.frame(cont_table)
names(ct_df) <- c("ATN", "Cluster", "Count")

p_heat <- ggplot(ct_df, aes(x = Cluster, y = ATN, fill = Count)) +
  geom_tile(color = "white", linewidth = 0.5) +
  geom_text(aes(label = Count), color = "white", fontface = "bold", size = 5) +
  scale_fill_viridis_c(option = "plasma") +
  labs(
    title = "ATN × Cluster Contingency Heatmap",
    subtitle = paste0("ARI = ", round(ari_val, 3), ", NMI = ", round(nmi_val, 3)),
    x = "K-Means Cluster",
    y = "ATN Profile"
  ) +
  theme_bw(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold"),
    axis.text.y = element_text(size = 10)
  )

ggsave("figures/figure6_panelA_contingency_heatmap.png", p_heat, width = 10, height = 8, dpi = 300)

# Stacked bar chart: ATN composition by cluster
ct_pct <- ct_df %>%
  group_by(Cluster) %>%
  mutate(Percent = 100 * Count / sum(Count))

p_stack <- ggplot(ct_pct, aes(x = Cluster, y = Percent, fill = ATN)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = paste0(round(Percent, 1), "%")),
            position = position_stack(vjust = 0.5),
            size = 3) +
  scale_fill_viridis_d(option = "turbo") +
  labs(
    title = "ATN Profile Composition by Cluster",
    y = "Percentage",
    x = "Cluster"
  ) +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(face = "bold"))

ggsave("figures/figure6_panelB_stacked_bars.png", p_stack, width = 10, height = 6, dpi = 300)
```


# Fuzzy K‑Means (FKM)

```{r}
# ---------- PREPARE DATA FOR FKM ----------
cat("\n=== FUZZY K-MEANS CLUSTERING ===\n")

# Create X_df with HHID_PN and log-transformed biomarkers
X_df <- final %>%
  select(HHID_PN, log_NfL, log_GFAP, log_AB42_40_ratio, log_pTau181_recode) %>%
  distinct() %>%
  na.omit()

cat("Sample size for FKM:", nrow(X_df), "\n")

# Extract numeric matrix (without HHID_PN) and scale
X <- X_df %>% select(-HHID_PN) %>% scale()

# ---------- FIT FUZZY K-MEANS ----------
set.seed(123)
fkm_model <- fcm(X, centers = 4, m = 2)

# Membership matrix (N x 4)
U <- fkm_model$u

# Hard cluster labels (assign each participant to cluster with highest membership)
hard_clusters <- apply(U, 1, which.max)

# ---------- CLUSTER SIZES ----------
cluster_sizes <- table(hard_clusters)
cat("\nFKM Cluster sizes:\n")
print(cluster_sizes)

# ---------- CLUSTER CENTROIDS (mean biomarker values per cluster) ----------
centroids <- X_df %>%
  mutate(Cluster = hard_clusters) %>%
  group_by(Cluster) %>%
  summarise(
    Mean_log_NfL = mean(log_NfL),
    Mean_log_GFAP = mean(log_GFAP),
    Mean_log_AB42_40 = mean(log_AB42_40_ratio),
    Mean_log_pTau181 = mean(log_pTau181_recode)
  )

# ---------- MEAN MEMBERSHIP PROBABILITY PER CLUSTER ----------
mean_membership <- colMeans(U)

# ---------- MEMBERSHIP ENTROPY (measure of fuzziness) ----------
# Higher entropy = more uncertain/fuzzy assignments
entropy <- -rowSums(U * log(U + 1e-12))  # entropy per participant
mean_entropy <- mean(entropy)            # overall fuzziness

cat("\nOverall mean entropy (fuzziness):", round(mean_entropy, 3), "\n")
cat("Interpretation: Higher entropy indicates more overlapping clusters\n")

# ---------- CREATE SUMMARY TABLE ----------
table_S18 <- tibble(
  Cluster = 1:4,
  Size = as.numeric(cluster_sizes),
  Mean_log_NfL = centroids$Mean_log_NfL,
  Mean_log_GFAP = centroids$Mean_log_GFAP,
  Mean_log_AB42_40 = centroids$Mean_log_AB42_40,
  Mean_log_pTau181 = centroids$Mean_log_pTau181,
  Mean_Membership = mean_membership,
  Overall_Entropy = mean_entropy  # Note: entropy is global, repeated for each row
)

# ---------- SAVE TABLE ----------
write.csv(table_S18, "tables/tableS18_fkm_summary_statistics.csv", row.names = FALSE)

cat("\nTable S18 saved: tables/tableS18_fkm_summary_statistics.csv\n")
print(table_S18)

cat("\n=== FUZZY K-MEANS COMPLETE ===\n\n")
```


# LONGITUDINAL COGNITIVE DECLINE ANALYSIS

```{r}
# ---- LONGITUDINAL COGNITIVE DECLINE ANALYSIS ------------

cat("\n=== LONGITUDINAL COGNITIVE DECLINE ANALYSIS ===\n")

# 2016 -> 2020 cognitive decline

long_2016_2020 <- final %>%
  filter(!is.na(Cog_Score_2016), !is.na(Cog_Score_2020)) %>%
  mutate(
    Cog_Decline_2016_2020 = Cog_Score_2016 - Cog_Score_2020
  )

cat("N with 2016 and 2020 cognition:", nrow(long_2016_2020), "\n")

# 1A. ATN predicting 2016→2020 decline
lm_atn_decline_2016_2020 <- lm(
  Cog_Decline_2016_2020 ~ ATN + PAGE + GENDER + RACE + SCHLYRS,
  data = long_2016_2020
)

summary(lm_atn_decline_2016_2020)
capture.output(summary(lm_atn_decline_2016_2020),
               file = "results/lm_atn_decline_2016_2020.txt")

# Clusters predicting 2016->2020 decline
long_2016_2020_clust <- long_2016_2020 %>%
  filter(!is.na(Cluster))

lm_cluster_decline_2016_2020 <- lm(
  Cog_Decline_2016_2020 ~ Cluster + PAGE + GENDER + RACE + SCHLYRS,
  data = long_2016_2020_clust
)

summary(lm_cluster_decline_2016_2020)
capture.output(summary(lm_cluster_decline_2016_2020),
               file = "results/lm_cluster_decline_2016_2020.txt")

# R-squared comparison (2016->2020)
model_compare_2016_2020 <- data.frame(
  Model = c("ATN-only", "Cluster-only", "ATN + Cluster"),
  R2 = c(
    summary(lm(Cog_Decline_2016_2020 ~ ATN, data = long_2016_2020))$r.squared,
    summary(lm(Cog_Decline_2016_2020 ~ Cluster, data = long_2016_2020_clust))$r.squared,
    summary(lm(Cog_Decline_2016_2020 ~ ATN + Cluster,
               data = long_2016_2020_clust))$r.squared
  )
)

write.csv(model_compare_2016_2020,
          "tables/table9_longitudinal_predictive_utility_2016_2020.csv",
          row.names = FALSE)

print(model_compare_2016_2020)


# 2018 -> 2020 cognitive decline

long_2018_2020 <- final %>%
  filter(!is.na(Cog_Score_2018), !is.na(Cog_Score_2020)) %>%
  mutate(
    Cog_Decline_2018_2020 = Cog_Score_2018 - Cog_Score_2020
  )

cat("N with 2018 and 2020 cognition:", nrow(long_2018_2020), "\n")

# ATN predicting 2018->2020 decline
lm_atn_decline_2018_2020 <- lm(
  Cog_Decline_2018_2020 ~ ATN + PAGE + GENDER + RACE + SCHLYRS,
  data = long_2018_2020
)

summary(lm_atn_decline_2018_2020)
capture.output(summary(lm_atn_decline_2018_2020),
               file = "results/lm_atn_decline_2018_2020.txt")

# Clusters predicting 2018->2020 decline
long_2018_2020_clust <- long_2018_2020 %>%
  filter(!is.na(Cluster))

lm_cluster_decline_2018_2020 <- lm(
  Cog_Decline_2018_2020 ~ Cluster + PAGE + GENDER + RACE + SCHLYRS,
  data = long_2018_2020_clust
)

summary(lm_cluster_decline_2018_2020)
capture.output(summary(lm_cluster_decline_2018_2020),
               file = "results/lm_cluster_decline_2018_2020.txt")

# R-squared comparison (2018->2020)
model_compare_2018_2020 <- data.frame(
  Model = c("ATN-only", "Cluster-only", "ATN + Cluster"),
  R2 = c(
    summary(lm(Cog_Decline_2018_2020 ~ ATN, data = long_2018_2020))$r.squared,
    summary(lm(Cog_Decline_2018_2020 ~ Cluster, data = long_2018_2020_clust))$r.squared,
    summary(lm(Cog_Decline_2018_2020 ~ ATN + Cluster,
               data = long_2018_2020_clust))$r.squared
  )
)

write.csv(model_compare_2018_2020,
          "tables/table9_longitudinal_predictive_utility_2018_2020.csv",
          row.names = FALSE)

print(model_compare_2018_2020)
```


# GENERATE MISSING SUPPLEMENTARY TABLES

```{r}
# ------ GENERATE MISSING SUPPLEMENTARY TABLES 

# TABLE S10: Seed Stability
cat("\n=== GENERATING TABLE S10 ===\n")
set.seed(123)
n_seeds <- 100
ari_seeds <- numeric(n_seeds)
cluster_sizes <- matrix(NA, nrow = n_seeds, ncol = optimal_k)

for (i in 1:n_seeds) {
  set.seed(i)
  km_seed <- kmeans(df_clust_scaled, centers = optimal_k, nstart = 50, iter.max = 500)
  ari_seeds[i] <- adjustedRandIndex(km$cluster, km_seed$cluster)
  cluster_sizes[i, ] <- table(km_seed$cluster)
}

tableS10 <- data.frame(
  Metric = c("Mean ARI", "SD ARI", "Min ARI", 
             paste0("Cluster ", 1:optimal_k, " mean (SD)")),
  Value = c(
    round(mean(ari_seeds), 3),
    round(sd(ari_seeds), 3),
    round(min(ari_seeds), 3),
    paste0(round(colMeans(cluster_sizes), 1), " (", 
           round(apply(cluster_sizes, 2, sd), 1), ")")
  )
)

write.csv(tableS10, "tables/tableS10_seed_stability.csv", row.names = FALSE)

# TABLE S11: Missing Data
cat("\n=== GENERATING TABLE S11 ===\n")
tableS11 <- data.frame(
  Variable = c("NfL", "GFAP", "Aβ42/40 ratio", "pTau181", "Age", "Gender", 
               "Race/Ethnicity", "Education", "APOE ε4", "Cognition 2016",
               "Cognition 2018", "Cognition 2020"),
  N_Missing = c(
    sum(is.na(final$NfL)),
    sum(is.na(final$GFAP)),
    sum(is.na(final$AB42_40_ratio)),
    sum(is.na(final$pTau181_recode)),
    sum(is.na(final$PAGE)),
    sum(is.na(final$GENDER)),
    sum(is.na(final$RACE_ETH)),
    sum(is.na(final$SCHLYRS)),
    NA,  # APOE not in dataset
    sum(is.na(final$Cog_Score_2016)),
    sum(is.na(final$Cog_Score_2018)),
    sum(is.na(final$Cog_Score_2020))
  ),
  Pct_Missing = round(c(
    mean(is.na(final$NfL)),
    mean(is.na(final$GFAP)),
    mean(is.na(final$AB42_40_ratio)),
    mean(is.na(final$pTau181_recode)),
    mean(is.na(final$PAGE)),
    mean(is.na(final$GENDER)),
    mean(is.na(final$RACE_ETH)),
    mean(is.na(final$SCHLYRS)),
    NA,
    mean(is.na(final$Cog_Score_2016)),
    mean(is.na(final$Cog_Score_2018)),
    mean(is.na(final$Cog_Score_2020))
  ) * 100, 1),
  Pattern = c(rep("Complete", 4), rep("MCAR", 4), NA, rep("MCAR", 3))
)

write.csv(tableS11, "tables/tableS11_missing_data.csv", row.names = FALSE)

cat("\n=== SUPPLEMENTARY TABLES COMPLETE ===\n")
```


# GFAP SENSITIVITY: COMPUTE ARI vs ATN

```{r}
# ---------- GFAP SENSITIVITY: COMPUTE ARI vs ATN --------------

cat("\n=== GFAP SENSITIVITY: ARI WITH ATN ===\n")

# Prepare ATN labels for participants with complete clustering
final_for_ari <- final %>%
  mutate(HHID_PN = trimws(as.character(HHID_PN))) %>%
  filter(!is.na(ATN))

# ---- 4-biomarker clustering (WITH GFAP) vs ATN ----
idx_4bio <- match(rownames(df_clust_scaled), final_for_ari$HHID_PN)
idx_4bio <- idx_4bio[!is.na(idx_4bio)]

ari_4bio_vs_atn <- adjustedRandIndex(
  km$cluster[!is.na(match(rownames(df_clust_scaled), final_for_ari$HHID_PN))],
  final_for_ari$ATN[idx_4bio]
)

cat("4-biomarker clustering vs ATN: ARI =", round(ari_4bio_vs_atn, 3), "\n")

# ---- 3-biomarker clustering (WITHOUT GFAP) vs ATN ----
idx_3bio <- match(rownames(df_scaled_no_gfap), final_for_ari$HHID_PN)
idx_3bio <- idx_3bio[!is.na(idx_3bio)]

ari_3bio_vs_atn <- adjustedRandIndex(
  km_no_gfap$cluster[!is.na(match(rownames(df_scaled_no_gfap), final_for_ari$HHID_PN))],
  final_for_ari$ATN[idx_3bio]
)

cat("3-biomarker clustering vs ATN: ARI =", round(ari_3bio_vs_atn, 3), "\n")

# ---- Calculate improvement ----
relative_improvement <- ((ari_3bio_vs_atn - ari_4bio_vs_atn) / ari_4bio_vs_atn) * 100

cat("Relative improvement:", round(relative_improvement, 1), "%\n\n")

# ---- Update GFAP sensitivity summary table ----
gfap_sensitivity_summary <- data.frame(
  Analysis = "GFAP Inclusion Sensitivity",
  ARI_4bio_vs_ATN = round(ari_4bio_vs_atn, 3),
  ARI_3bio_vs_ATN = round(ari_3bio_vs_atn, 3),
  Relative_Improvement_Pct = round(relative_improvement, 1),
  ARI_with_vs_without = round(ari_gfap_sensitivity, 3),
  Silhouette_with_GFAP = round(mean_sil_width, 3),
  Silhouette_without_GFAP = round(mean_sil_no_gfap, 3),
  N_with_GFAP = nrow(df_clust_scaled),
  N_without_GFAP = nrow(df_scaled_no_gfap),
  Interpretation = paste0(
    "GFAP contributes ~", 
    round((1 - ari_gfap_sensitivity) * 100, 0),
    "% of discordance"
  )
)

write.csv(gfap_sensitivity_summary, 
          "tables/tableS12_gfap_sensitivity.csv", 
          row.names = FALSE)

cat("Updated tableS12_gfap_sensitivity.csv with ARI vs ATN values\n")
```


# VAE with PCA Comparison

```{r}
# ---- Variational Autoencoder (VAE) with PCA Comparison ----

x <- df_clust_scaled
input_dim <- ncol(x)
latent_dim <- 2
intermediate_dim <- 64

# Build VAE
vae_model <- keras_model_custom(name = "vae", function(self) {
  
  # Encoder layers
  self$encoder_input <- layer_input(shape = input_dim, name = "encoder_input")
  self$enc_dense <- layer_dense(units = intermediate_dim, activation = "relu", name = "enc_dense")
  self$z_mean_layer <- layer_dense(units = latent_dim, name = "z_mean")
  self$z_log_var_layer <- layer_dense(units = latent_dim, name = "z_log_var")
  
  # Decoder layers
  self$dec_input <- layer_input(shape = latent_dim, name = "z_sampling")
  self$dec_dense <- layer_dense(units = intermediate_dim, activation = "relu", name = "dec_dense")
  self$dec_output <- layer_dense(units = input_dim, activation = "linear", name = "dec_output")
  
  # Encoder forward
  encode <- function(x) {
    h <- self$enc_dense(x)
    z_mean <- self$z_mean_layer(h)
    z_log_var <- self$z_log_var_layer(h)
    epsilon <- k_random_normal(shape = k_shape(z_mean))
    z <- z_mean + k_exp(0.5 * z_log_var) * epsilon
    list(z, z_mean, z_log_var)
  }
  
  # Decoder forward
  decode <- function(z) {
    h_dec <- self$dec_dense(z)
    self$dec_output(h_dec)
  }
  
  # Call method
  self$call <- function(inputs, training = FALSE) {
    res <- encode(inputs)
    z <- res[[1]]
    z_mean <- res[[2]]
    z_log_var <- res[[3]]
    reconstructed <- decode(z)
    
    self$z_mean <- z_mean
    self$z_log_var <- z_log_var
    self$z <- z
    
    reconstructed
  }
  
  # Custom train_step
  self$train_step <- function(data) {
    with(tf$GradientTape() %as% tape, {
      reconstructed <- self(data, training = TRUE)
      recon_loss <- tf$reduce_mean(tf$reduce_sum(tf$math$square(data - reconstructed), axis = 1L))
      kl_loss <- -0.5 * tf$reduce_mean(tf$reduce_sum(1 + self$z_log_var - tf$math$square(self$z_mean) - tf$math$exp(self$z_log_var), axis = 1L))
      total_loss <- recon_loss + kl_loss
    })
    gradients <- tape$gradient(total_loss, self$trainable_variables)
    self$optimizer$apply_gradients(purrr::transpose(list(gradients, self$trainable_variables)))
    list(loss = total_loss, recon_loss = recon_loss, kl_loss = kl_loss)
  }
  
  # Encoder/decoder models
  self$encoder <- keras_model(
    inputs = self$encoder_input,
    outputs = list(
      self$z_mean_layer(self$enc_dense(self$encoder_input)),
      self$z_log_var_layer(self$enc_dense(self$encoder_input))
    ),
    name = "encoder_model"
  )
  
  self$decoder <- keras_model(
    inputs = self$dec_input,
    outputs = self$dec_output(self$dec_dense(self$dec_input)),
    name = "decoder_model"
  )
})

vae_model %>% compile(optimizer = "adam")

# Callbacks
callbacks_list <- list(
  callback_early_stopping(monitor = "loss", patience = 10, restore_best_weights = TRUE),
  callback_reduce_lr_on_plateau(monitor = "loss", factor = 0.5, patience = 5, min_lr = 1e-6)
)

# Train VAE
cat("\nTraining VAE...\n")
history <- vae_model %>% fit(
  x,
  epochs = 50,
  batch_size = 32,
  callbacks = callbacks_list,
  verbose = 1
)

# Save models
vae_model$encoder %>% compile(optimizer = "adam", loss = "mse")
vae_model$decoder %>% compile(optimizer = "adam", loss = "mse")
save_model_hdf5(vae_model$encoder, "results/encoder_model.h5")
save_model_hdf5(vae_model$decoder, "results/decoder_model.h5")

# Extract latent representations
encoded_means <- predict(vae_model$encoder, x)[[1]]
latent_vae <- as.data.frame(encoded_means)
colnames(latent_vae) <- c("z1", "z2")
latent_vae$HHID_PN <- df_clust$HHID_PN
latent_vae$ATN <- final$ATN[match(latent_vae$HHID_PN, final$HHID_PN)]
latent_vae$Cluster <- final$Cluster[match(latent_vae$HHID_PN, final$HHID_PN)]


# ---- VAE Explainability with SHAP (Optional) ----
try({
  shap <- import("shap")
  np <- import("numpy")

  cat("\n=== VAE EXPLAINABILITY (SHAP) ===\n")

  x_np <- np$array(df_clust_scaled)

  explainer <- shap$KernelExplainer(
    model = function(z) vae_model$predict(z),
    data = x_np[1:200, ]
  )

  shap_values <- explainer$shap_values(x_np[1:500, ])

  saveRDS(shap_values, "results/vae_shap_values.rds")

  png("figures/vae_shap_summary.png", width = 1200, height = 900)
  shap$summary_plot(shap_values, x_np[1:500, ])
  dev.off()

}, silent = TRUE)


# ---- PCA for Comparison ----
pca_full <- prcomp(df_clust_scaled, center = FALSE, scale. = FALSE)
latent_pca <- as.data.frame(pca_full$x[, 1:2])
colnames(latent_pca) <- c("PC1", "PC2")
latent_pca$HHID_PN <- df_clust$HHID_PN
latent_pca$ATN <- final$ATN[match(latent_pca$HHID_PN, final$HHID_PN)]
latent_pca$Cluster <- final$Cluster[match(latent_pca$HHID_PN, final$HHID_PN)]

# Variance explained
var_pca <- summary(pca_full)$importance[2, 1:2]
cat("\nPCA variance explained (first 2 components):", 
    round(sum(var_pca) * 100, 1), "%\n")

# ---- Quantitative Comparison: Cluster Separation ----
# Silhouette in VAE space
latent_vae_complete <- latent_vae[!is.na(latent_vae$Cluster), ]
dist_vae <- dist(as.matrix(latent_vae_complete[, c("z1", "z2")]))
sil_vae <- silhouette(as.integer(as.factor(latent_vae_complete$Cluster)), dist_vae)
mean_sil_vae <- mean(sil_vae[, 3])

# Silhouette in PCA space
latent_pca_complete <- latent_pca[!is.na(latent_pca$Cluster), ]
dist_pca <- dist(as.matrix(latent_pca_complete[, c("PC1", "PC2")]))
sil_pca <- silhouette(as.integer(as.factor(latent_pca_complete$Cluster)), dist_pca)
mean_sil_pca <- mean(sil_pca[, 3])

# Silhouette in original space (4D)
dist_original <- dist(df_clust_scaled)
sil_original <- silhouette(km$cluster, dist_original)
mean_sil_original <- mean(sil_original[, 3])

comparison_metrics <- data.frame(
  Method = c("Original (4D)", "PCA (2D)", "VAE (2D)"),
  Mean_Silhouette = c(mean_sil_original, mean_sil_pca, mean_sil_vae),
  Variance_Explained = c(100, sum(var_pca) * 100, NA)
)

cat("\n=== DIMENSIONALITY REDUCTION COMPARISON ===\n")
print(comparison_metrics)
cat("\n")

write.csv(comparison_metrics, "tables/tableS15_dimensionality_reduction_comparison.csv",
          row.names = FALSE)

# ---- Side-by-side Visualization ----
p1 <- ggplot(latent_pca, aes(x = PC1, y = PC2, color = ATN)) +
  geom_point(alpha = 0.6, size = 2) +
  labs(
    title = "PCA Projection",
    subtitle = paste0("Variance explained: ", round(sum(var_pca) * 100, 1), "%")
  ) +
  theme_bw(base_size = 12) +
  theme(plot.title = element_text(face = "bold"))

p2 <- ggplot(latent_vae, aes(x = z1, y = z2, color = ATN)) +
  geom_point(alpha = 0.6, size = 2) +
  labs(
    title = "VAE Latent Space",
    subtitle = paste0("Mean silhouette: ", round(mean_sil_vae, 3))
  ) +
  theme_bw(base_size = 12) +
  theme(plot.title = element_text(face = "bold"))

p_combined <- grid.arrange(p1, p2, ncol = 2, 
                           top = "Dimensionality Reduction Comparison: PCA vs VAE")

ggsave("figures/figure7_panelA_pca_vs_vae.png", p_combined,
       width = 14, height = 6, dpi = 300)

# By cluster
p3 <- ggplot(latent_pca, aes(x = PC1, y = PC2, color = Cluster)) +
  geom_point(alpha = 0.6, size = 2) +
  labs(title = "PCA: Colored by Cluster") +
  theme_bw(base_size = 12)

p4 <- ggplot(latent_vae, aes(x = z1, y = z2, color = Cluster)) +
  geom_point(alpha = 0.6, size = 2) +
  labs(title = "VAE: Colored by Cluster") +
  theme_bw(base_size = 12)

p_combined_cluster <- grid.arrange(p3, p4, ncol = 2)
ggsave("figures/figure7_panelB_pca_vs_vae_by_cluster.png", p_combined_cluster,
       width = 14, height = 6, dpi = 300)

# ---- VAE Validation ----

# Correlation with original biomarkers
latent_vae$z1 <- as.numeric(latent_vae$z1)
latent_vae$z2 <- as.numeric(latent_vae$z2)

matched_idx <- match(latent_vae$HHID_PN, final$HHID_PN)
bio_mat <- final[matched_idx, c("NfL", "GFAP", "AB42_40_ratio", "pTau181_recode")]
cog_vec <- final$Cog_Score_2016[matched_idx]

corr_matrix <- cor(
  cbind(latent_vae[, c("z1", "z2")], bio_mat, cognition = cog_vec),
  use = "pairwise.complete.obs"
)

cat("\n=== LATENT DIMENSION CORRELATIONS ===\n")
print(round(corr_matrix, 3))
cat("\n")

write.csv(round(corr_matrix, 3), "tables/tableS5_vae_latent_correlations.csv")

# Heatmap
png("figures/figureS5_vae_latent_correlations.png", width = 800, height = 600)
corrplot(corr_matrix, method = "color", type = "upper", 
         tl.col = "black", tl.srt = 45,
         title = "Correlations: VAE Latents vs Biomarkers",
         mar = c(0,0,2,0))
dev.off()

# Training history
loss_df <- data.frame(
  epoch = 1:length(history$metrics$loss),
  total_loss = history$metrics$loss,
  recon_loss = history$metrics$recon_loss,
  kl_loss = history$metrics$kl_loss
)

write.csv(loss_df, "tables/tableS7_vae_training_history.csv", row.names = FALSE)

loss_melt <- reshape2::melt(loss_df, id.vars = "epoch")

p_training <- ggplot(loss_melt, aes(x = epoch, y = value, color = variable)) +
  geom_line(linewidth = 1.2) +
  facet_wrap(~ variable, scales = "free_y", ncol = 1) +
  labs(
    title = "VAE Training Curves",
    x = "Epoch",
    y = "Loss"
  ) +
  theme_bw(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold"),
    legend.position = "none"
  )

ggsave("figures/figureS6_vae_training_curves.png", p_training,
       width = 8, height = 10, dpi = 300)

# Save latent representations
save(latent_vae, latent_pca, file = "results/latent_representations.RData")
```


# Internal Validation

```{r}
# ---- Internal Validation ----

# Train/test split (70/30)
set.seed(123)
idx <- sample(seq_len(nrow(final)), size = floor(0.7 * nrow(final)))
train <- final[idx, ]
test  <- final[-idx, ]

cat("\n=== TRAIN/TEST SPLIT ===\n")
cat("Training set size:", nrow(train), "\n")
cat("Test set size:", nrow(test), "\n\n")

 # ROC analysis on test set
 eval_roc_on_test <- function(train, test, biomarker,
                              outcome = "Dementia_IMP_2016",
                              case = "Dementia", control = "Normal") {
   vars <- c(outcome, biomarker, "PAGE", "GENDER", "RACE", "SCHLYRS")
   tr <- train[, vars] %>% na.omit()
   te <- test[, vars] %>% na.omit()
   tr <- tr[tr[[outcome]] %in% c(case, control), ]
   te <- te[te[[outcome]] %in% c(case, control), ]
   tr[[outcome]] <- factor(tr[[outcome]], levels = c(control, case))
   te[[outcome]] <- factor(te[[outcome]], levels = c(control, case))

   fit <- glm(as.formula(paste(outcome, "~", biomarker,
                               "+ PAGE + GENDER + RACE + SCHLYRS")),
              data = tr, family = "binomial")

   pi_hat <- predict(fit, newdata = te, type = "response")

   roc_obj <- pROC::roc(te[[outcome]], pi_hat,
                        levels = c(control, case), direction = "<")

   list(roc = roc_obj, auc = pROC::auc(roc_obj), n = nrow(te))
 }

 roc_test_results <- lapply(biomarkers, function(b) eval_roc_on_test(train, test, b))
 names(roc_test_results) <- biomarkers

# Test set performance table
 test_performance <- data.frame(
   Biomarker = c("NfL", "GFAP", "Aβ42/40", "pTau181"),
   Test_AUC = sapply(roc_test_results, function(x) round(x$auc, 3)),
   Test_N = sapply(roc_test_results, function(x) x$n)
 )

cat("\n=== TEST SET PERFORMANCE ===\n")
print(test_performance)
cat("\n")

write.csv(test_performance, "tables/test_set_performance.csv", row.names = FALSE)

# Bar plot
p_test_auc <- ggplot(test_performance, aes(x = Biomarker, y = Test_AUC, fill = Biomarker)) +
  geom_bar(stat = "identity") +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "gray50") +
  coord_flip() +
  labs(
    title = "Test Set Performance: AUC by Biomarker",
    x = "Biomarker",
    y = "AUC"
  ) +
  scale_fill_brewer(palette = "Set2") +
  theme_bw(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold"),
    legend.position = "none"
  )

ggsave("figures/test_auc_bars.png", p_test_auc, width = 8, height = 6, dpi = 300)

# ---- Clustering Stability ----
cat("\nAssessing clustering stability (50 bootstrap iterations)...\n")

set.seed(123)
B <- 50
ari_scores <- numeric(B)
base_labels <- km$cluster

for (b in 1:B) {
  idx_sub <- sample(seq_len(nrow(df_clust_scaled)),
                   size = floor(0.8 * nrow(df_clust_scaled)),
                   replace = FALSE)

  km_sub <- kmeans(df_clust_scaled[idx_sub, , drop = FALSE],
                   centers = optimal_k, nstart = 50, iter.max = 300)

  # Map back to full dataset
  cent <- km_sub$centers
  d_all <- as.matrix(dist(rbind(cent, df_clust_scaled)))
  d_to_cent <- d_all[1:nrow(cent), (nrow(cent)+1):nrow(d_all)]
  assign_full <- apply(d_to_cent, 2, which.min)

  ari_scores[b] <- adjustedRandIndex(base_labels, assign_full)
}

stability_summary <- data.frame(
  Metric = c("Mean ARI", "Median ARI", "Min ARI", "Max ARI", "SD ARI"),
  Value = c(mean(ari_scores), median(ari_scores), min(ari_scores),
            max(ari_scores), sd(ari_scores))
)

cat("\n=== CLUSTERING STABILITY (Bootstrap ARI) ===\n")
print(stability_summary)
cat("\n")

write.csv(stability_summary, "tables/clustering_stability.csv", row.names = FALSE)

# Histogram
ari_df <- data.frame(ARI = ari_scores)

p_stability <- ggplot(ari_df, aes(x = ARI)) +
  geom_histogram(bins = 20, fill = "darkorange", color = "white", alpha = 0.8) +
  geom_vline(xintercept = mean(ari_scores), linetype = "dashed",
             color = "red", linewidth = 1) +
  annotate("text", x = mean(ari_scores), y = Inf,
           label = paste0("Mean = ", round(mean(ari_scores), 3)),
           vjust = 1.5, color = "red", fontface = "bold") +
  labs(
    title = "Clustering Stability via Bootstrap Resampling",
    subtitle = paste0("50 iterations with 80% subsampling (k = ", optimal_k, ")"),
    x = "Adjusted Rand Index vs. Base Clustering",
    y = "Count"
  ) +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(face = "bold"))

ggsave("figures/cluster_stability_ari.png", p_stability, width = 10, height = 6, dpi = 300)
```


# Sensitivity Analyses

```{r}
# ---- Sensitivity Analyses ----

cat("\n=== SENSITIVITY ANALYSES ===\n\n")

# ---- Alternative ATN Cutoffs (Quartile-based) ----
cat("1. Testing alternative ATN cutoffs (quartile-based)...\n")

final_q <- final %>%
  mutate(
    A_q = ifelse(log_AB42_40_ratio < quantile(log_AB42_40_ratio, 0.25, na.rm = TRUE), 
                 "A+", "A-"),
    T_q = ifelse(log_pTau181_recode > quantile(log_pTau181_recode, 0.75, na.rm = TRUE), 
                 "T+", "T-"),
    N_q = ifelse(log_NfL > quantile(log_NfL, 0.75, na.rm = TRUE), 
                 "N+", "N-"),
    ATN_q = paste(A_q, T_q, N_q, sep = "/")
  )

# Compare literature-based vs quartile-based ATN
atn_comparison <- final_q %>%
  filter(!is.na(ATN), !is.na(ATN_q)) %>%
  select(ATN, ATN_q)

ari_atn <- adjustedRandIndex(atn_comparison$ATN, atn_comparison$ATN_q)
nmi_atn <- NMI(as.character(atn_comparison$ATN), as.character(atn_comparison$ATN_q))

cat("  Agreement between literature-based and quartile-based ATN:\n")
cat("    ARI =", round(ari_atn, 3), "\n")
cat("    NMI =", round(nmi_atn, 3), "\n\n")

# Crosstab
atn_crosstab <- table(atn_comparison$ATN, atn_comparison$ATN_q)
write.csv(as.data.frame.matrix(atn_crosstab),
          "tables/tableS12_atn_cutoff_sensitivity_crosstab.csv")
```


# Alternative VAE Architectures

```{r}
# ---- Alternative VAE Architectures ----
cat("Testing alternative VAE architectures...\n")

# Helper function for simple VAE training
train_vae_simple <- function(x, latent_dim = 3, intermediate_dim = 64, 
                             epochs = 30, batch_size = 32) {
  input_dim <- ncol(x)
  
  # Encoder
  enc_input <- layer_input(shape = input_dim)
  h_enc <- enc_input %>% layer_dense(units = intermediate_dim, activation = "relu")
  z_mean <- h_enc %>% layer_dense(units = latent_dim, name = "z_mean")
  z_log_var <- h_enc %>% layer_dense(units = latent_dim, name = "z_log_var")
  
  # Sampling
# Sampling function
sampling <- function(args) {
  z_mean <- args[[1]]
  z_log_var <- args[[2]]
  epsilon <- k_random_normal(shape = k_shape(z_mean))
  z_mean + k_exp(0.5 * z_log_var) * epsilon
}

# Correct Lambda layer
z <- layer_lambda(f = sampling, name = "z_sampling")(list(z_mean, z_log_var))
  
  # Decoder
  dec_input <- layer_input(shape = latent_dim)
  h_dec <- dec_input %>% layer_dense(units = intermediate_dim, activation = "relu")
  dec_output <- h_dec %>% layer_dense(units = input_dim, activation = "linear")
  
  encoder <- keras_model(enc_input, z_mean, name = "encoder")
  decoder <- keras_model(dec_input, dec_output, name = "decoder")
  
  # VAE
  vae_output <- decoder(z)
  vae <- keras_model(enc_input, vae_output, name = "vae")
  
  # Compile and train
  vae %>% compile(optimizer = "adam", loss = "mse")
  history <- vae %>% fit(x, x, epochs = epochs, batch_size = batch_size, 
                        verbose = 0, validation_split = 0.2)
  
  list(encoder = encoder, decoder = decoder, vae = vae, 
       history = history, final_loss = tail(history$metrics$loss, 1))
}

# Test different latent dimensions
latent_dims <- c(2, 3, 4, 5)
vae_comparison <- data.frame(
  Latent_Dim = integer(),
  Final_Loss = numeric(),
  Final_Val_Loss = numeric()
)

for (ld in latent_dims) {
  cat("  Training VAE with latent_dim =", ld, "...\n")
  set.seed(123)
  vae_temp <- train_vae_simple(x, latent_dim = ld, epochs = 30)
  
  vae_comparison <- rbind(vae_comparison, data.frame(
    Latent_Dim = ld,
    Final_Loss = tail(vae_temp$history$metrics$loss, 1),
    Final_Val_Loss = tail(vae_temp$history$metrics$val_loss, 1)
  ))
}

cat("\n  VAE Architecture Comparison:\n")
print(vae_comparison)
cat("\n")

write.csv(vae_comparison, "tables/tableS15_vae_latent_dimensions.csv", row.names = FALSE)

# Plot
vae_comp_melt <- reshape2::melt(vae_comparison, id.vars = "Latent_Dim")

p_vae_sens <- ggplot(vae_comp_melt, aes(x = factor(Latent_Dim), y = value, 
                                        fill = variable)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(
    title = "VAE Performance: Alternative Latent Dimensions",
    x = "Latent Dimension",
    y = "Final Loss",
    fill = "Loss Type"
  ) +
  scale_fill_manual(values = c("steelblue", "darkorange"),
                   labels = c("Training", "Validation")) +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(face = "bold"))

ggsave("figures/figureS15_vae_latent_dimensions.png", p_vae_sens,
       width = 10, height = 6, dpi = 300)
```


# Alternative Clustering Solutions (k-1, k+1)

```{r}
# ---- Alternative Clustering Solutions (k-1, k+1) ----
cat("Testing alternative number of clusters...\n")

k_alternatives <- c(optimal_k - 1, optimal_k, optimal_k + 1)
k_comparison <- data.frame(
  K = integer(),
  Mean_Silhouette = numeric(),
  Total_WSS = numeric()
)

for (k_test in k_alternatives) {
  if (k_test < 2) next  # Skip if k < 2
  
  set.seed(123)
  km_test <- kmeans(df_clust_scaled, centers = k_test, nstart = 50, iter.max = 300)
  sil_test <- silhouette(km_test$cluster, dist(df_clust_scaled))
  
  k_comparison <- rbind(k_comparison, data.frame(
    K = k_test,
    Mean_Silhouette = mean(sil_test[, 3]),
    Total_WSS = km_test$tot.withinss
  ))
}

cat("\n  Clustering Solutions Comparison:\n")
print(k_comparison)
cat("\n")

write.csv(k_comparison, "tables/tableS17_cluster_quality_by_k.csv", row.names = FALSE)
```


# Robustness to Missing Data Pattern

```{r}
# ---- Robustness to Missing Data Pattern ----
cat("Assessing robustness to missing data...\n")

missing_pattern <- final %>%
  mutate(
    Has_NfL = !is.na(NfL),
    Has_GFAP = !is.na(GFAP),
    Has_AB = !is.na(AB42_40_ratio),
    Has_pTau = !is.na(pTau181_recode),
    Complete_Biomarkers = Has_NfL & Has_GFAP & Has_AB & Has_pTau
  )

missing_summary <- missing_pattern %>%
  summarise(
    N_Total = n(),
    N_Complete = sum(Complete_Biomarkers),
    Pct_Complete = mean(Complete_Biomarkers) * 100,
    
    N_Missing_NfL = sum(!Has_NfL),
    N_Missing_GFAP = sum(!Has_GFAP),
    N_Missing_AB = sum(!Has_AB),
    N_Missing_pTau = sum(!Has_pTau)
  )

cat("\n  Missing Data Summary:\n")
print(missing_summary)
cat("\n")

write.csv(missing_summary, "tables/tableS11_missing_data_summary.csv", row.names = FALSE)

# Check if missingness is related to demographics/outcomes
missing_pattern_test <- missing_pattern %>%
  group_by(Complete_Biomarkers) %>%
  summarise(
    N = n(),
    Mean_Age = mean(PAGE, na.rm = TRUE),
    Pct_Female = mean(GENDER == "Female", na.rm = TRUE) * 100,
    Mean_Cog = mean(Cog_Score_2016, na.rm = TRUE)
  )

write.csv(missing_pattern_test, "tables/tableS11_missing_by_demographics.csv",
          row.names = FALSE)
```


# Comprehensive missing data table

```{r}
# Comprehensive missing data table
tableS11 <- final %>%
  summarise(
    across(
      c(NfL, GFAP, AB42_40_ratio, pTau181_recode, PAGE, GENDER, RACE, SCHLYRS,
        Cog_Score_2016, Cog_Score_2018, Cog_Score_2020),
      list(
        N_Missing = ~sum(is.na(.)),
        Pct_Missing = ~round(100 * mean(is.na(.)), 1)
      ),
      .names = "{.col}_{.fn}"
    )
  ) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value") %>%
  separate(Variable, into = c("Variable", "Metric"), sep = "_(?=[^_]+$)") %>%
  pivot_wider(names_from = Metric, values_from = Value)

write.csv(tableS11, "tables/tableS11_missing_data_complete.csv", row.names = FALSE)
```


# Summary of All Sensitivity Analyses

```{r}
# ---- Summary of All Sensitivity Analyses ----
sensitivity_summary <- data.frame(
  Analysis = c(
    "Alternative ATN cutoffs (quartile)",
    "VAE latent dimension selection",
    "Alternative clustering (k±1)",
    "Missing data pattern"
  ),
  Finding = c(
    paste0("ARI = ", round(ari_atn, 3), " with quartile cutoffs"),
    paste0("Optimal latent_dim = ", vae_comparison$Latent_Dim[which.min(vae_comparison$Final_Val_Loss)]),
    paste0("Best k = ", k_comparison$K[which.max(k_comparison$Mean_Silhouette)]),
    paste0(round(missing_summary$Pct_Complete, 1), "% with complete biomarkers")
  ),
  Interpretation = c(
    ifelse(ari_atn > 0.6, "High agreement", "Moderate agreement"),
    "Supports dimensionality choice",
    ifelse(k_comparison$K[which.max(k_comparison$Mean_Silhouette)] == optimal_k, 
           "Confirms k selection", "Suggests alternative k"),
    "Missing not at random - potential bias"
  )
)

knitr::kable(sensitivity_summary, 
             caption = "Summary of Sensitivity Analyses")

write.csv(sensitivity_summary, "tables/sensitivity_analyses_summary.csv", 
          row.names = FALSE)

cat("\n=== SENSITIVITY ANALYSES COMPLETE ===\n\n")
```


# Comprehensive Sensitivity Summary

```{r}
# Create Table 10: Comprehensive Sensitivity Summary
table10_sensitivity <- data.frame(
  Analysis = c(
    "GFAP-Excluded Clustering",
    "k=3 vs k=4 Comparison",
    "Cluster 4 Stability",
    "Distance Metric Sensitivity",
    "Alternative ATN Cutoffs",
    "Missing Data Assessment",
    "VAE Latent Dimensions"
  ),
  Finding = c(
    paste0("ARI = 0.187 (3-bio) vs 0.119 (4-bio); +57% improvement"),
    paste0("Silhouette: k=3 = 0.547, k=4 = 0.474; k=4 preserves extremes"),
    paste0("Jaccard = 0.779; Manhattan ARI = 0.946"),
    paste0("ARI = 0.946 (Manhattan vs Euclidean)"),
    paste0("15-20% reclassification under ±10% threshold variation"),
    paste0(round(mean(!is.na(final$Cluster)) * 100, 1), "% complete cases"),
    paste0("2D optimal (validation loss = 1.548)")
  ),
  Interpretation = c(
    "GFAP contributes ~1/3 discordance; binary vs continuous = 2/3",
    "k=4 provides superior biological resolution",
    "Cluster 4 is stable, not artifact",
    "Results robust to distance metric",
    "ATN moderately sensitive to cutoffs",
    "Minimal bias from missingness",
    "Parsimonious representation optimal"
  )
)

write.csv(table10_sensitivity, "tables/table10_sensitivity_analyses_summary.csv", row.names = FALSE)
```



# Visualization: Silhouette Comparison Across Representations

```{r}
# ---- FIGURE 7, PANEL B — SILHOUETTE COMPARISON ACROSS REPRESENTATIONS ---- 

cat("\n=== Computing Silhouette Comparison Across Representations ===\n")

# ---------- PCA EMBEDDING ----------
pca_full <- prcomp(df_clust_scaled, center = FALSE, scale. = FALSE)

latent_pca <- as.data.frame(pca_full$x[, 1:2])
colnames(latent_pca) <- c("PC1", "PC2")

latent_pca$HHID_PN <- df_clust$HHID_PN
latent_pca$Cluster <- final$Cluster[match(latent_pca$HHID_PN, final$HHID_PN)]
latent_pca_complete <- latent_pca[!is.na(latent_pca$Cluster), ]

# ---------- VAE EMBEDDING (fallback if latent_vae not already created) ----------
if (!exists("latent_vae")) {
  message("latent_vae not found — computing VAE latent space now...")

  x <- df_clust_scaled
  input_dim <- ncol(x)
  latent_dim <- 2
  intermediate_dim <- 64

  enc_input <- layer_input(shape = input_dim)
  h_enc <- enc_input %>% layer_dense(units = intermediate_dim, activation = "relu")
  z_mean <- h_enc %>% layer_dense(units = latent_dim)
  encoder_temp <- keras_model(enc_input, z_mean)

  encoded_means <- predict(encoder_temp, x)
  latent_vae <- as.data.frame(encoded_means)
  colnames(latent_vae) <- c("z1", "z2")

  latent_vae$HHID_PN <- df_clust$HHID_PN
  latent_vae$Cluster <- final$Cluster[match(latent_vae$HHID_PN, final$HHID_PN)]
}

latent_vae_complete <- latent_vae[!is.na(latent_vae$Cluster), ]

# ---------- SILHOUETTE COMPUTATION ----------
sil_original <- silhouette(km$cluster, dist(df_clust_scaled))

dist_pca <- dist(as.matrix(latent_pca_complete[, c("PC1", "PC2")]))
sil_pca <- silhouette(as.integer(as.factor(latent_pca_complete$Cluster)), dist_pca)

dist_vae <- dist(as.matrix(latent_vae_complete[, c("z1", "z2")]))
sil_vae <- silhouette(as.integer(as.factor(latent_vae_complete$Cluster)), dist_vae)

# ---------- ASSEMBLE SILHOUETTE DATAFRAME ----------
sil_df <- bind_rows(
  data.frame(
    Silhouette = sil_original[, 3],
    Space = "Original 4D",
    Cluster = factor(km$cluster)
  ),
  data.frame(
    Silhouette = sil_pca[, 3],
    Space = "PCA 2D",
    Cluster = factor(latent_pca_complete$Cluster)
  ),
  data.frame(
    Silhouette = sil_vae[, 3],
    Space = "VAE 2D",
    Cluster = factor(latent_vae_complete$Cluster)
  )
)

sil_df$Space <- factor(sil_df$Space, levels = c("Original 4D", "PCA 2D", "VAE 2D"))

# ---------- PAIRWISE STATISTICAL COMPARISONS ----------
p_original_pca <- wilcox.test(
  sil_df$Silhouette[sil_df$Space == "Original 4D"],
  sil_df$Silhouette[sil_df$Space == "PCA 2D"]
)$p.value

p_original_vae <- wilcox.test(
  sil_df$Silhouette[sil_df$Space == "Original 4D"],
  sil_df$Silhouette[sil_df$Space == "VAE 2D"]
)$p.value

p_pca_vae <- wilcox.test(
  sil_df$Silhouette[sil_df$Space == "PCA 2D"],
  sil_df$Silhouette[sil_df$Space == "VAE 2D"]
)$p.value

label_orig_pca <- ifelse(p_original_pca < 0.001, "p < 0.001",
                         paste0("p = ", format(round(p_original_pca, 3), nsmall = 3)))
label_orig_vae <- ifelse(p_original_vae < 0.001, "p < 0.001",
                         paste0("p = ", format(round(p_original_vae, 3), nsmall = 3)))
label_pca_vae <- ifelse(p_pca_vae < 0.001, "p < 0.001",
                        paste0("p = ", format(round(p_pca_vae, 3), nsmall = 3)))

# ---------- PUBLICATION-READY PLOT ----------
p_panelB_fixed <- ggplot(sil_df, aes(x = Space, y = Silhouette, fill = Space)) +
  geom_boxplot(alpha = 0.8, outlier.shape = NA, linewidth = 0.7) +
  geom_jitter(aes(color = Cluster), width = 0.15, alpha = 0.6, size = 1.8) +
  scale_fill_manual(values = c("Original 4D" = "#1b9e77",
                               "PCA 2D"     = "#d95f02",
                               "VAE 2D"     = "#7570b3")) +
  scale_color_manual(values = c("1" = "#1b9e77",
                                "2" = "#d95f02",
                                "3" = "#7570b3",
                                "4" = "#e7298a")) +
  labs(
    title = "Panel B. Silhouette Comparison Across Representations",
    x = "Representation Space",
    y = "Silhouette Width",
    fill = "Space",
    color = "Cluster"
  ) +
  theme_bw(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    legend.position = "right",
    panel.grid.minor = element_blank()
  ) +
  annotate("segment", x = 1, xend = 2, y = 0.82, yend = 0.82) +
  annotate("text", x = 1.5, y = 0.84, label = label_orig_pca, fontface = "bold") +
  annotate("segment", x = 1, xend = 3, y = 0.90, yend = 0.90) +
  annotate("text", x = 2, y = 0.92, label = label_orig_vae, fontface = "bold") +
  annotate("segment", x = 2, xend = 3, y = 0.82, yend = 0.82) +
  annotate("text", x = 2.5, y = 0.84, label = label_pca_vae, fontface = "bold")

# ---------- SAVE FIGURE ----------
ggsave("figures/figure7_panelB_silhouette_comparison.png",
       p_panelB_fixed, width = 10, height = 7, dpi = 600)

cat("\n=== Figure 7 Panel B saved to figures/figure7_panelB_silhouette_comparison.png ===\n")
```


# Method Comparison Summary Table

```{r}
# ---- Comprehensive Method Comparison ----

cat("\n=== CREATING COMPREHENSIVE METHOD COMPARISON ===\n")

# Gather all metrics
method_comparison <- data.frame(
  Method = c("ATN Framework", "K-Means Clustering", "VAE Latent Space", "PCA"),
  
  Approach = c(
    "Theory-driven",
    "Data-driven (unsupervised)",
    "Data-driven (deep learning)",
    "Data-driven (linear)"
  ),
  
  N_Groups = c(
    length(unique(final$ATN)),
    optimal_k,
    NA,
    NA
  ),
  
  Mean_Silhouette = c(
    NA,
    mean_sil_original,
    mean_sil_vae,
    mean_sil_pca
  ),
  
  Variance_Explained = c(
    NA,
    NA,
    NA,
    round(sum(var_pca) * 100, 1)
  ),
  
  Cognitive_Association = c(
    format_p(kw_cog$p.value),
    format_p(kruskal.test(Cog_Score_2016 ~ Cluster, 
                         data = final_tab)$p.value),
    NA,
    NA
  ),
  
  Interpretability = c("High", "Medium", "Low", "High"),
  
  Advantages = c(
    "Clinical relevance; validated cutoffs",
    "Discovers natural groupings; no assumptions",
    "Non-linear relationships; dimensionality reduction",
    "Simple; variance maximization"
  ),
  
  Limitations = c(
    "Binary cutoffs lose information; platform-specific",
    "Requires k selection; assumes spherical clusters",
    "Black box; requires tuning",
    "Linear only; may miss non-linear patterns"
  )
)

knitr::kable(method_comparison, 
             caption = "Comprehensive Comparison of Classification Approaches")

write.csv(method_comparison, "tables/method_comparison_comprehensive.csv", 
          row.names = FALSE)
```


# Cross-Method Cluster Agreement (Sankey Diagram)

```{r}
# Figure S9: Cross-Method Cluster Agreement (Sankey Diagram)

# Original flow data
flow_data <- data.frame(
  source = c(rep("k-means_C1", 3), rep("k-means_C2", 3), 
             rep("k-means_C3", 3), rep("k-means_C4", 3)),
  target = c("GMM_C1", "GMM_C2", "GMM_C3",
             "GMM_C4", "GMM_C5", "GMM_C6",
             "GMM_C6", "GMM_C7", "GMM_C8",
             "GMM_C9", "Hierarchical_C1", "Hierarchical_C2"),
  value = c(45, 3, 3, 365, 200, 318, 2800, 400, 279, 12, 1, 1)
)

# 1. Create nodes table
nodes <- data.frame(name = unique(c(flow_data$source, flow_data$target)))

# 2. Convert source/target to numeric indices
links <- flow_data %>%
  mutate(
    source = match(source, nodes$name) - 1,
    target = match(target, nodes$name) - 1
  )

# 3. Create Sankey diagram widget
p_sankey <- sankeyNetwork(
  Links = links,
  Nodes = nodes,
  Source = "source",
  Target = "target",
  Value = "value",
  NodeID = "name",
  fontSize = 14,
  nodeWidth = 30,
  sinksRight = FALSE
)

# 4. Save as HTML
saveWidget(p_sankey, "figures/figureS9_sankey.html", selfcontained = TRUE)

# 5. Convert HTML → PNG
webshot("figures/figureS9_sankey.html",
        file = "figures/figureS9_sankey.png",
        vwidth = 1200,
        vheight = 900,
        delay = 0.5)
```


# CUTOFF SENSITIVITY ANALYSIS

```{r}
# ----------- TABLE S19: CUTOFF SENSITIVITY ANALYSIS --------------

cat("\n=== GENERATING TABLE S19: CUTOFF SENSITIVITY ===\n")

# Define alternative cutoff scenarios
cutoff_scenarios <- list(
  # Scenario 1: Primary (reference)
  primary = list(AB = 0.067, pTau = 2.2, NfL = 20),
  
  # Scenario 2: Lower thresholds (more sensitive)
  lower = list(AB = 0.060, pTau = 2.0, NfL = 18),
  
  # Scenario 3: Higher thresholds (more specific)
  higher = list(AB = 0.074, pTau = 2.4, NfL = 22)
)

# Count reclassifications for each biomarker
reclassification_summary <- data.frame(
  Biomarker = c("Aβ42/40 ratio", "p-Tau181", "NfL"),
  Primary_Cutoff = c("< 0.067", "> 2.2 pg/mL", "> 20 pg/mL"),
  Alternative_Range = c("0.060 – 0.074", "2.0 – 2.4 pg/mL", "18 – 22 pg/mL"),
  Estimated_Reclassification_Pct = numeric(3),
  Common_Transitions = c(
    "A+ ↔ A- (borderline cases)",
    "T+ ↔ T- (intermediate pathology)",
    "N+ ↔ N- (mild neurodegeneration)"
  )
)

# Estimate reclassification for Aβ42/40 ratio
ab_borderline <- final %>%
  filter(AB42_40_ratio >= cutoff_scenarios$lower$AB & 
         AB42_40_ratio <= cutoff_scenarios$higher$AB)
reclassification_summary$Estimated_Reclassification_Pct[1] <- 
  round(100 * nrow(ab_borderline) / nrow(final), 1)

# Estimate reclassification for p-Tau181
ptau_borderline <- final %>%
  filter(pTau181_recode >= cutoff_scenarios$lower$pTau & 
         pTau181_recode <= cutoff_scenarios$higher$pTau)
reclassification_summary$Estimated_Reclassification_Pct[2] <- 
  round(100 * nrow(ptau_borderline) / nrow(final), 1)

# Estimate reclassification for NfL
nfl_borderline <- final %>%
  filter(NfL >= cutoff_scenarios$lower$NfL & 
         NfL <= cutoff_scenarios$higher$NfL)
reclassification_summary$Estimated_Reclassification_Pct[3] <- 
  round(100 * nrow(nfl_borderline) / nrow(final), 1)

write.csv(reclassification_summary, 
          "tables/tableS19_cutoff_reclassification.csv", 
          row.names = FALSE)

cat("\nTable S19 saved: tables/tableS19_cutoff_reclassification.csv\n")
print(reclassification_summary)
```


# Agreement/concordance matrix

```{r}
# Agreement/concordance matrix
agreement_matrix <- data.frame(
  Comparison = c(
    "ATN vs K-Means",
    "ATN vs ATN_quartile",
    "K-Means (4D) vs PCA (2D)",
    "K-Means (4D) vs VAE (2D)"
  ),
  ARI = c(
    ari_val,
    ari_atn,
    adjustedRandIndex(
      km$cluster,
      kmeans(pca_full$x[, 1:2], centers = optimal_k, nstart = 50)$cluster
    ),
    adjustedRandIndex(
      df_clust_clean$Cluster[match(latent_vae_complete$HHID_PN, 
                                    df_clust_clean$HHID_PN)],
      kmeans(as.matrix(latent_vae_complete[, c("z1", "z2")]), 
             centers = optimal_k, nstart = 50)$cluster
    )
  ),
  NMI = c(
    nmi_val,
    nmi_atn,
    NA,
    NA
  ),
  Interpretation = c(
    ifelse(ari_val > 0.3, "Moderate agreement", "Low agreement"),
    ifelse(ari_atn > 0.6, "High agreement", "Moderate agreement"),
    "Compares projection methods",
    "Compares projection methods"
  )
)

knitr::kable(agreement_matrix, digits = 3,
             caption = "Agreement Between Classification Methods")

write.csv(agreement_matrix, "tables/method_agreement_matrix.csv", row.names = FALSE)

cat("\n=== METHOD COMPARISON COMPLETE ===\n\n")
```



# Enhanced Visualization

### I did all of the visualization, but I am redoing all of them to enhance versions for publication ubgrade and save them in TIFF


```{r}
# =============================================================================
# DIRECTORY SETUP
# =============================================================================

dir.create("figures/png", recursive = TRUE, showWarnings = FALSE)
dir.create("figures/tiff", recursive = TRUE, showWarnings = FALSE)
dir.create("figures/supplementary/png", recursive = TRUE, showWarnings = FALSE)
dir.create("figures/supplementary/tiff", recursive = TRUE, showWarnings = FALSE)

# =============================================================================
# SAVE FUNCTION (DEFINED ONCE)
# =============================================================================

save_dual_format <- function(plot_object, filename_base,
                             width = 10, height = 6,
                             dpi = 300, supplementary = FALSE) {

  base_dir <- if (supplementary) "figures/supplementary" else "figures"

  # PNG
  ggsave(
    filename = file.path(base_dir, "png", paste0(filename_base, ".png")),
    plot = plot_object,
    width = width, height = height, dpi = dpi, bg = "white"
  )

  # TIFF
  ggsave(
    filename = file.path(base_dir, "tiff", paste0(filename_base, ".tiff")),
    plot = plot_object,
    width = width, height = height, dpi = 600,
    compression = "lzw", bg = "white"
  )

  cat("✓ Saved:", filename_base, "\n")
}

# =============================================================================
# MAIN FIGURES
# =============================================================================

cat("\n=== GENERATING MAIN FIGURES ===\n\n")

# FIGURE 1 — CONSORT
if (file.exists("figures/consort_flowchart.svg")) {
  fig1 <- image_read("figures/consort_flowchart.svg")
  image_write(fig1, "figures/tiff/figure1_consort_flowchart.tiff",
              format = "tiff", density = 600, compression = "lzw")
  cat(" Figure 1 converted\n")
}

# FIGURE 2 — BIOMARKERS + ATN
stopifnot(inherits(g, "ggplot"))
stopifnot(inherits(g_atn, "ggplot"))

save_dual_format(g, "figure2_panelA_biomarker_violins")
save_dual_format(g_atn, "figure2_panelB_atn_profiles")

figure2_combined <- g / g_atn +
  plot_annotation(tag_levels = "A",
                  title = "Plasma Biomarker Distributions and ATN Profile Prevalence")

save_dual_format(figure2_combined, "figure2_combined", height = 12)

# FIGURE 3 — CLUSTER NUMBER
save_dual_format(p_elbow, "figure3_panelA_elbow", width = 12, height = 8)
save_dual_format(p_sil, "figure3_panelB_silhouette", width = 12, height = 8)

p_nb <- ggplot(nb_df, aes(factor(k), freq)) +
  geom_col(fill = "steelblue") +
  labs(title = "Panel C. Optimal Number of Clusters (NbClust)",
       x = "Number of Clusters", y = "Number of Indices Supporting k") +
  theme_bw(base_size = 20)

save_dual_format(p_nb, "figure3_panelC_nbclust", width = 12, height = 8)

figure3_combined <- (p_elbow | p_sil) / p_nb
save_dual_format(figure3_combined, "figure3_combined", width = 14, height = 12)

# FIGURE 4 — CENTROIDS
save_dual_format(p_centroids, "figure4_cluster_centroids_heatmap", width = 8, height = 6)

# FIGURE 5 — VALIDATION
save_dual_format(sil_plot, "figure5_panelA_silhouette")
save_dual_format(p_cluster, "figure5_panelB_pca_biplot", width = 12, height = 8)

figure5_combined <- sil_plot / p_cluster + plot_annotation(tag_levels = "A")
save_dual_format(figure5_combined, "figure5_combined", height = 14)

# FIGURE 6 — ATN AGREEMENT
save_dual_format(p_heat, "figure6_panelA_contingency_heatmap", width = 10, height = 8)
save_dual_format(p_stack, "figure6_panelB_stacked_bars", width = 10, height = 6)

figure6_combined <- p_heat / p_stack + plot_annotation(tag_levels = "A")
save_dual_format(figure6_combined, "figure6_combined", height = 14)

# FIGURE 7 — Panel A (PCA vs VAE)

p_combined_obj <- arrangeGrob(p1, p2, ncol = 2,
                              top = "Dimensionality Reduction Comparison: PCA vs VAE")
p_combined_gg <- ggplotify::as.ggplot(p_combined_obj)

save_dual_format(p_combined_gg,
                 "figure7_panelA_pca_vs_vae",
                 width = 14, height = 6)


# FIGURE 7 — Panel B (Silhouette Comparison)

save_dual_format(p_panelB_fixed,
                 "figure7_panelB_silhouette_comparison",
                 width = 10, height = 7)


# FIGURE 7 — Combined Figure (Panels A + B)

figure7_combined <- p_combined_gg / p_panelB_fixed + 
                    plot_annotation(tag_levels = "A")

save_dual_format(figure7_combined,
                 "figure7_combined",
                 width = 14, height = 14)

# =============================================================================
# SUPPLEMENTARY FIGURES
# =============================================================================

cat("\n=== GENERATING SUPPLEMENTARY FIGURES ===\n\n")

save_dual_format(qq_grid, "figureS1_biomarker_qq_plots", supplementary = TRUE, height = 10)
save_dual_format(gmm_sil_plot, "figureS2_gmm_bic_silhouette", supplementary = TRUE)

save_dual_format(p_stab, "figureS3_cluster_stability_jaccard",
                 supplementary = TRUE, width = 12, height = 8)

save_dual_format(figureS4, "figureS4_tsne_umap", supplementary = TRUE, width = 14, height = 6)

# =============================================================================
# FIGURE S5 — VAE Latent Correlations (corrplot)
# =============================================================================

# PNG (high‑resolution, matches TIFF geometry)
png("figures/supplementary/png/figureS5_vae_latent_correlations.png",
    width = 4800, height = 3600, res = 600)

corrplot(
  corr_matrix,
  method = "color",
  type = "upper",
  tl.col = "black",
  tl.srt = 45,
  title = "Correlations: VAE Latents vs Biomarkers",
  mar = c(0, 0, 2, 0)
)

dev.off()

# TIFF (publication‑quality, correct scaling)
tiff("figures/supplementary/tiff/figureS5_vae_latent_correlations.tiff",
     width = 8, height = 6, units = "in",
     res = 600, compression = "lzw")

corrplot(
  corr_matrix,
  method = "color",
  type = "upper",
  tl.col = "black",
  tl.srt = 45,
  title = "Correlations: VAE Latents vs Biomarkers",
  mar = c(0, 0, 2, 0)
)

dev.off()

save_dual_format(p_training, "figureS6_vae_training_curves", supplementary = TRUE, height = 10)
save_dual_format(figure_S7, "figureS7_k3_vs_k4", supplementary = TRUE, width = 14, height = 12)
save_dual_format(figure_S8, "figureS8_ATN_cutoffs", supplementary = TRUE, width = 16, height = 14)

# S9 — Sankey
if (file.exists("figures/figureS9_sankey.png")) {
  file.copy("figures/figureS9_sankey.png",
            "figures/supplementary/png/figureS9_sankey.png",
            overwrite = TRUE)
  fig_s9 <- image_read("figures/figureS9_sankey.png")
  image_write(fig_s9, "figures/supplementary/tiff/figureS9_sankey.tiff",
              format = "tiff", density = 600, compression = "lzw")
}

# =============================================================================
# ADDITIONAL FIGURES (ROC, Calibration, DCA, Misc)
# =============================================================================

cat("\n=== GENERATING ADDITIONAL TIFF FIGURES ===\n\n")

# ---- ROC curves per biomarker ----
if (exists("roc_results") && is.list(roc_results)) {
  for (b in names(roc_results)) {
    png_path <- file.path("figures", paste0("roc_", b, ".png"))
    if (file.exists(png_path)) {
      fig <- image_read(png_path)
      image_write(
        fig,
        path = file.path("figures/tiff", paste0("roc_", b, ".tiff")),
        format = "tiff",
        density = 600,
        compression = "lzw"
      )
      cat("✓ Converted ROC:", b, "\n")
    }
  }
}

# ---- Calibration plots ----
biomarkers <- c("log_NfL", "log_GFAP", "log_AB42_40_ratio", "log_pTau181_recode")

for (b in biomarkers) {
  png_path <- file.path("figures", paste0("calibration_", b, ".png"))
  if (file.exists(png_path)) {
    fig <- image_read(png_path)
    image_write(
      fig,
      path = file.path("figures/tiff", paste0("calibration_", b, ".tiff")),
      format = "tiff",
      density = 600,
      compression = "lzw"
    )
    cat("✓ Converted Calibration:", b, "\n")
  }
}

# ---- DCA plots ----
for (b in biomarkers) {
  png_path <- file.path("figures", paste0("dca_", b, ".png"))
  if (file.exists(png_path)) {
    fig <- image_read(png_path)
    image_write(
      fig,
      path = file.path("figures/tiff", paste0("dca_", b, ".tiff")),
      format = "tiff",
      density = 600,
      compression = "lzw"
    )
    cat("✓ Converted DCA:", b, "\n")
  }
}

# ---- Stratified ROC curves ----
for (b in biomarkers) {

  # Race-stratified
  png_path_race <- file.path("figures", paste0("roc_race_composite_", b, ".png"))
  if (file.exists(png_path_race)) {
    fig <- image_read(png_path_race)
    image_write(
      fig,
      path = file.path("figures/tiff", paste0("roc_race_composite_", b, ".tiff")),
      format = "tiff",
      density = 600,
      compression = "lzw"
    )
    cat("✓ Converted Race ROC:", b, "\n")
  }

  # Sex-stratified
  png_path_sex <- file.path("figures", paste0("roc_sex_composite_", b, ".png"))
  if (file.exists(png_path_sex)) {
    fig <- image_read(png_path_sex)
    image_write(
      fig,
      path = file.path("figures/tiff", paste0("roc_sex_composite_", b, ".tiff")),
      format = "tiff",
      density = 600,
      compression = "lzw"
    )
    cat("✓ Converted Sex ROC:", b, "\n")
  }
}

# ---- Miscellaneous figures ----
misc_figures <- c(
  "biomarker_violin_plot",
  "nbclust_recommendation",
  "sensitivity_k_silhouette_plot",
  "cluster_stability_ari",
  "test_auc_bars",
  "supplementary_biomarker_boxplots_by_cluster"
)

for (fig_name in misc_figures) {
  png_path <- file.path("figures", paste0(fig_name, ".png"))
  if (file.exists(png_path)) {
    fig <- image_read(png_path)
    image_write(
      fig,
      path = file.path("figures/tiff", paste0(fig_name, ".tiff")),
      format = "tiff",
      density = 600,
      compression = "lzw"
    )
    cat("✓ Converted Misc Figure:", fig_name, "\n")
  }
}

# =============================================================================
# MANIFEST
# =============================================================================

main_tiffs <- list.files("figures/tiff", pattern = "\\.tiff$", full.names = TRUE)
supp_tiffs <- list.files("figures/supplementary/tiff", pattern = "\\.tiff$", full.names = TRUE)

manifest <- data.frame(
  File = c(main_tiffs, supp_tiffs),
  Size_MB = round(file.size(c(main_tiffs, supp_tiffs)) / 1024^2, 2)
)

write.csv(manifest, "figures/TIFF_MANIFEST.csv", row.names = FALSE)

cat("\n=== COMPLETE ===\n")

```


# Final Summary Report

```{r}
# ---- Generate Final Summary Report ----

cat("\n========================================\n")
cat("FINAL ANALYSIS SUMMARY\n")
cat("========================================\n\n")

# Sample characteristics
cat("1. SAMPLE CHARACTERISTICS\n")
cat("   Total participants:", nrow(final), "\n")
cat("   With complete biomarkers:", nrow(df_clust), "\n")
cat("   With complete data (biomarkers + covariates):", nrow(final_tab), "\n\n")

# ATN distribution
cat("2. ATN FRAMEWORK\n")
cat("   Number of ATN profiles:", length(unique(final$ATN)), "\n")
cat("   Most common profile:", names(which.max(table(final$ATN))),
    "(n =", max(table(final$ATN)), ")\n")
cat("   Least common profile:", names(which.min(table(final$ATN))),
    "(n =", min(table(final$ATN)), ")\n\n")

# Clustering results
cat("3. DATA-DRIVEN CLUSTERING\n")
cat("   Optimal k:", optimal_k, "\n")
cat("   Mean silhouette width:", round(mean_sil_original, 3), "\n")
cat("   Stability (mean bootstrap ARI):", round(mean(ari_scores), 3), "\n\n")

# Agreement
cat("4. ATN-CLUSTER AGREEMENT\n")
cat("   Adjusted Rand Index:", round(ari_val, 3), "\n")
cat("   Normalized Mutual Information:", round(nmi_val, 3), "\n")
cat("   Interpretation:",
    ifelse(ari_val > 0.3, "Moderate agreement", "Low agreement"), "\n\n")

# Predictive performance
cat("5. BIOMARKER PREDICTIVE PERFORMANCE (Test Set)\n")
for (i in 1:nrow(test_performance)) {
  cat("   ", test_performance$Biomarker[i], "AUC:",
      test_performance$Test_AUC[i], "\n")
}
cat("\n")

# Dimensionality reduction
cat("6. DIMENSIONALITY REDUCTION\n")
cat("   PCA (2D) variance explained:", round(sum(var_pca) * 100, 1), "%\n")
cat("   VAE mean silhouette:", round(mean_sil_vae, 3), "\n")
cat("   PCA mean silhouette:", round(mean_sil_pca, 3), "\n\n")

# Key findings
cat("7. KEY FINDINGS\n")
cat("   • ATN and data-driven clusters show",
    ifelse(ari_val > 0.3, "moderate", "limited"), "alignment\n")
cat("   • Best performing biomarker:",
    test_performance$Biomarker[which.max(test_performance$Test_AUC)],
    "(AUC =", max(test_performance$Test_AUC), ")\n")
cat("   • Clustering is",
    ifelse(mean(ari_scores) > 0.7, "highly stable",
           ifelse(mean(ari_scores) > 0.5, "moderately stable", "unstable")),
    "\n")
cat("   • VAE",
    ifelse(mean_sil_vae > mean_sil_pca, "outperforms", "underperforms"),
    "PCA for cluster separation\n\n")

cat("8. OUTPUT FILES GENERATED\n")
cat("   Tables:", length(list.files("tables", pattern = "\\.csv$")), "files\n")
cat("   Figures:", length(list.files("figures", pattern = "\\.png$")), "files\n")
cat("   Results:", length(list.files("results")), "files\n\n")

cat("========================================\n")
cat("ANALYSIS COMPLETE\n")
cat("========================================\n\n")
```


# Save summary report

```{r}
# Save summary report
summary_report <- list(
  sample_size = nrow(final),
  complete_biomarkers = nrow(df_clust),
  optimal_k = optimal_k,
  mean_silhouette = mean_sil_original,
  ari_atn_cluster = ari_val,
  nmi_atn_cluster = nmi_val,
  best_biomarker = test_performance$Biomarker[which.max(test_performance$Test_AUC)],
  best_auc = max(test_performance$Test_AUC),
  clustering_stability = mean(ari_scores),
  pca_variance = sum(var_pca),
  vae_silhouette = mean_sil_vae,
  pca_silhouette = mean_sil_pca
)

saveRDS(summary_report, "results/analysis_summary.rds")

# Create a summary table
final_summary_table <- data.frame(
  Metric = names(summary_report),
  Value = unlist(summary_report)
)

write.csv(final_summary_table, "tables/analysis_summary.csv", row.names = FALSE)
```
